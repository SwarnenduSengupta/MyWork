---
title: "RegressionExample"
author: "Dr.B"
date: "Sunday, May 17, 2015"
output:
  html_document:
    toc: yes
  pdf_document:
    number_sections: yes
    toc: yes
---

```{r,warnings=FALSE, messages=FALSE}
# Load functions
source('functions.R')

##Load libraries
library(car)
library(gvlma)
library(corrgram)
library(ggplot2)
library(e1071)
library(fBasics)
library(gmodels)
```

Load the mtcars dataset.
```{r}
data(mtcars)

#create a new columns
#mtcars$wthp <- mtcars$wt*mtcars$hp
#mtcars$LOGMPG <- log(mtcars$mpg)

#Clean the data
mtcars<-cleanit(mtcars)

# count blanks remove blanks
colSums(!is.na(mtcars))
# mtcars <- na.omit(mtcars)
#colSums(!is.na(mtcars))
```

Compute the correlation matrix with all 11 varaiables. 
```{r}
cor(mtcars)
```

Show corr gram
```{r}
corrgram(mtcars, order=TRUE, lower.panel=panel.shade,
  upper.panel=panel.pie, text.panel=panel.txt,
  main="Car Milage Data in PC2/PC1 Order")
```

```{r}
corrgram(mtcars, order=TRUE, lower.panel=panel.ellipse,
  upper.panel=panel.pts, text.panel=panel.txt,
  diag.panel=panel.minmax, 
   main="Car Milage Data in PC2/PC1 Order")
```

Trim down the correlation matrix to first column and sort it
```{r}
cor.out <- sort(cor(mtcars)[,1])
round(cor.out, 3)
```

For more efficient analysis, transform the following 5 variables into factors:
```{r}
mtcars$gear <- factor(mtcars$gear,levels=c(3,4,5),labels=c("3-gears","4-gears","5-gears"))
mtcars$cyl <- factor(mtcars$cyl,levels=c(4,6,8),labels=c("4-cyl","6-cyl","8-cyl")) 
mtcars$am <- factor(mtcars$am,levels=c(0,1),labels=c("Automatic","Manual"))
mtcars$vs <- factor(mtcars$vs,,levels=c(0,1),labels=c("V-Engine","Straight-Engine"))
mtcars$carb <- factor(mtcars$carb,levels=c(1,2,3,4,6,8),labels=c("1-carb","2-carbs","3-carbs","4-carbs","6-carbs","8-carbs"))
```

##Numerical Summaries
```{r,warning=FALSE,message=FALSE}
str(mtcars)
summary(mtcars)
```

###Univariate
```{r,warning=FALSE,message=FALSE}
mean(mtcars$mpg)
stem(mtcars$mpg)
fivenum(mtcars$mpg,na.rm=TRUE)

# Continous data use type=6
quantile(mtcars$mpg, probs = c(5,10,25,50,75)/100, type=6)
IQR(mtcars$mpg,type=6)

# Factors
table(mtcars$carb)
table(mtcars$gear)
table(mtcars$cyl) 
table(mtcars$am)
table(mtcars$vs)
```

##Bivariate
```{r,warning=FALSE,message=FALSE}
aggregate(mpg~am, data = mtcars, mean)
aggregate(mpg~cyl, data = mtcars, mean)
aggregate(mpg~vs, data = mtcars, mean)
aggregate(mpg~gear, data = mtcars, mean)
aggregate(mpg~carb, data = mtcars, mean)

#Factors
table(mtcars$am, mtcars$vs)
table(mtcars$am, mtcars$cyl)
ftable(table(mtcars$am, mtcars$car))


# Test for Association/Correlation Between Paired Samples
cor.test(~mpg+wt, data=mtcars)
cor.test(~mpg+hp, data=mtcars)
cor.test(~mpg+qsec, data=mtcars)
cor.test(~mpg+disp, data=mtcars)
cor.test(~mpg+drat, data=mtcars)
```

##Graphical Summaries

###Univariate
```{r,warning=FALSE,message=FALSE}
pairs(mtcars)

#Check Outliers of outcome
outliers(mtcars$mpg)
plot(outliers(mtcars$mpg)$Z)
```

###Bivariate
```{r,warning=FALSE,message=FALSE}
##Set printing
par(mfrow=c(1,2))

## scatterplot
plot(mpg~wt,mtcars)
abline(lm(mpg~wt,mtcars),col="red")
plot(mpg~hp,mtcars)
abline(lm(mpg~hp,mtcars),col="red")

## boxplot
boxplot(mpg~am, data = mtcars,col = c("Green", "Yellow"),xlab = "Transmission Type",ylab = "Miles per Gallon",main = "MPG by Transmission Type")
abline(h=mean(mtcars$mpg),col="red")

boxplot(mpg~cyl, data = mtcars,col = c("Green", "Yellow","Red"),xlab = "Cylinders Type",ylab = "Miles per Gallon",main = "MPG by Nuber of Cylinders")
abline(h=mean(mtcars$mpg),col="red")

boxplot(mpg~vs, data = mtcars,col = c("Green", "Yellow"),xlab = "Engine Type",ylab = "Miles per Gallon",main = "MPG by Engine Type")
abline(h=mean(mtcars$mpg),col="red")

boxplot(mpg~gear, data = mtcars,col = c("Green", "Yellow","Blue"),xlab = "Forward Gears Type",ylab = "Miles per Gallon",main = "MPG by Number of Forward Gears")
abline(h=mean(mtcars$mpg),col="red")

boxplot(mpg~carb, data = mtcars,col = c("Green3", "Yellow","Red","Blue","Green","Gray"),xlab = "Carbs Type",ylab = "Miles per Gallon",main = "MPG by Number of Carbs")
abline(h=mean(mtcars$mpg),col="red")


#Factors
par(mfrow=c(1,2))
barplot(table(mtcars$am, mtcars$vs))
barplot(table(mtcars$am, mtcars$cyl))
barplot(table(mtcars$am, mtcars$car))
barplot(table(mtcars$am, mtcars$gear))


##Set printing
par(mfrow=c(1,2))
qplot(wt, mpg, color=factor(am), data=mtcars, geom=c("point", "smooth"))
qplot(hp, mpg, color=factor(am), data=mtcars, geom=c("point", "smooth"))
qplot(drat, mpg, color=factor(am), data=mtcars, geom=c("point", "smooth"))
qplot(disp, mpg, color=factor(am), data=mtcars, geom=c("point", "smooth"))
qplot(qsec, mpg, color=factor(am), data=mtcars, geom=c("point", "smooth"))
qplot(wt, mpg, color=factor(cyl), data=mtcars, geom=c("point", "smooth"))
qplot(hp, mpg, color=factor(cyl), data=mtcars, geom=c("point", "smooth"))
qplot(drat, mpg, color=factor(cyl), data=mtcars, geom=c("point", "smooth"))
qplot(disp, mpg, color=factor(cyl), data=mtcars, geom=c("point", "smooth"))
qplot(qsec, mpg, color=factor(cyl), data=mtcars, geom=c("point", "smooth"))
qplot(wt, mpg, color=factor(vs), data=mtcars, geom=c("point", "smooth"))
qplot(hp, mpg, color=factor(vs), data=mtcars, geom=c("point", "smooth"))
qplot(drat, mpg, color=factor(vs), data=mtcars, geom=c("point", "smooth"))
qplot(disp, mpg, color=factor(vs), data=mtcars, geom=c("point", "smooth"))
qplot(qsec, mpg, color=factor(vs), data=mtcars, geom=c("point", "smooth"))
qplot(wt, mpg, color=factor(gear), data=mtcars, geom=c("point", "smooth"))
qplot(hp, mpg, color=factor(gear), data=mtcars, geom=c("point", "smooth"))
qplot(drat, mpg, color=factor(gear), data=mtcars, geom=c("point", "smooth"))
qplot(disp, mpg, color=factor(gear), data=mtcars, geom=c("point", "smooth"))
qplot(qsec, mpg, color=factor(gear), data=mtcars, geom=c("point", "smooth"))
qplot(wt, mpg, color=factor(carb), data=mtcars, geom=c("point", "smooth"))
qplot(hp, mpg, color=factor(carb), data=mtcars, geom=c("point", "smooth"))
qplot(drat, mpg, color=factor(carb), data=mtcars, geom=c("point", "smooth"))
qplot(disp, mpg, color=factor(carb), data=mtcars, geom=c("point", "smooth"))
qplot(qsec, mpg, color=factor(carb), data=mtcars, geom=c("point", "smooth"))
```



Base regression model containing only am as the predictor variable.
```{r,warning=FALSE,message=TRUE}
basemodel <- lm(mpg ~ am, data = mtcars)
summary(basemodel)
#coefficients(basemodel) # model coefficients
#confint(basemodel, level=0.95) # CIs for model parameters 
cbind(Coef = coef(basemodel), confint(basemodel)) ## coefficients and 95% CI together
fitted(basemodel) # predicted values
anova(basemodel) # anova table 
vcov(basemodel) # covariance matrix for model parameters 
```

Inclusive model that included all variables as predictors of mpg.
```{r,warning=FALSE,message=FALSE}
inclusivemodel <- lm(mpg ~ ., data = mtcars)
summary(inclusivemodel)
#coefficients(inclusivemodel) # model coefficients
#confint(inclusivemodel, level=0.95) # CIs for model parameters 
cbind(Coef = coef(inclusivemodel), confint(inclusivemodel)) ## coefficients and 95% CI together
fitted(inclusivemodel) # predicted values
anova(inclusivemodel) # anova table 
vcov(inclusivemodel) # covariance matrix for model parameters 
```

Compare the base model and the inclusive model
```{r}
anova(basemodel, inclusivemodel)
```

Perform stepwise model selection in order to select significant predictors for the final, best model. The step function will perform this selection by calling lm repeatedly to build multiple regression models and select the best variables from them using both forward selection and backward elimination methods using AIC algorithm. This  ensures that the useful variables are included in the model while omitting ones that do not contribute significantly to predicting mpg.
```{r}
bestmodel <- step(inclusivemodel, direction = "both")

summary(bestmodel)
#coefficients(bestmodel) # model coefficients
#confint(bestmodel, level=0.95) # CIs for model parameters 
cbind(Coef = coef(bestmodel), confint(bestmodel)) ## coefficients and 95% CI together
fitted(bestmodel) # predicted values
anova(bestmodel) # anova table 
vcov(bestmodel) # covariance matrix for model parameters 
```


Compare the base model and the best model
```{r}
anova(basemodel, bestmodel)
```

Plot
```{r}
plot(basemodel)
plot(inclusivemodel)
plot(bestmodel)
```


##Model Assumptions
The assumptions for simple linear regression are:

        Y relates to X by a linear regression model
        the errors are independent and identically normally distributed with mean zero and common variance
        
Violations:

        In the linear regression model:
                linearity (e.g. quadratic relationship or higher order terms)
                
In the residual assumptions:

        non-normal distribution
        non-constant variances
        dependence
        outliers
        
Checks:

        look at plot of residuals vs. X
        look at plot of residuals vs. fitted values
        look at residuals Q-Q norm plot
        
##Checking assumptions graphically

Residuals vs. X
```{r}
##Set printing
par(mfrow=c(1,2))

plot(resid(basemodel))
plot(resid(inclusivemodel))
plot(resid(bestmodel))
```

Residuals vs. fitted values
```{r}
##Set printing
par(mfrow=c(1,2))

plot(resid(basemodel)~fitted(basemodel))
plot(resid(inclusivemodel)~fitted(inclusivemodel))
plot(resid(bestmodel)~fitted(bestmodel))
```        

Residuals QQ Plot
```{r,warning=FALSE,error=FALSE,message=FALSE}
##Set printing
par(mfrow=c(1,2))

qqnorm(resid(basemodel))
qqline(resid(basemodel))
qqnorm(resid(inclusivemodel))
qqline(resid(inclusivemodel))
qqnorm(resid(bestmodel))
qqline(resid(bestmodel))

# Plots empirical quantiles of a variable, or of studentized residuals from a linear model, against theoretical quantiles of a comparison distribution.
qqPlot(basemodel)
#qqPlot(inclusivemodel)
qqPlot(bestmodel)
```

I computed regression diagnostics of the best model to identify leverage points. I computed the top three points in each case of influence measures. The data points with the most leverage in the fit are identfied by hatvalues().
```{r}
tail(sort(hatvalues(bestmodel)),3)
tail(sort(hatvalues(basemodel)),3)
tail(sort(hatvalues(inclusivemodel)),3)
```

The data points that influence the model coefficients the most are given by the dfbetas() function.
```{r}
tail(sort(dfbetas(bestmodel)[,6]),3)
tail(sort(dfbetas(basemodel)[,1]),3)
tail(sort(dfbetas(inclusivemodel)[,6]),3)
```

The models of vehicles identifyied above are the same models identified with the residual plots

##Influence Index Plot
Provides index plots of Cook's distances, leverages, Studentized residuals, and outlier significance levels for a regression object.
```{r,warning=FALSE,error=FALSE,message=FALSE}
influenceIndexPlot(model = basemodel, id.n = 5)
#influenceIndexPlot(model = inclusivemodel, id.n = 5)
influenceIndexPlot(model = bestmodel, id.n = 5)
```


##dfbeta and dfbetas Index Plots
These functions display index plots of dfbeta (effect on coefficients of deleting each observation in turn) and dfbetas (effect on coefficients of deleting each observation in turn, standardized by a deleted estimate of the coefficient standard error). In the plot of dfbeta, horizontal lines are drawn at 0 and +/- one standard error; in the plot of dfbetas, horizontal lines are drawn and 0 and +/- 1.
```{r,warning=FALSE,error=FALSE,message=FALSE}
## dfbeta and dfbetas Index Plots
## id.n  Number of points to be identified. If set to zero, no points are identified.
dfbetasPlots(model = basemodel, id.n = 5)
dfbetasPlots(model = inclusivemodel, id.n = 5)
dfbetasPlots(model = bestmodel, id.n = 5)
## Easier to see if the y-axis is fixed
dfbetasPlots(model = basemodel, id.n = 5, ylim = c(-1,1)*1.5)
dfbetasPlots(model = inclusivemodel, id.n = 5, ylim = c(-1,1)*1.5)
dfbetasPlots(model = bestmodel, id.n = 5, ylim = c(-1,1)*1.5)
```

##Regression Influence Plot
This function creates a "bubble" plot of Studentized residuals by hat values, with the areas of the circles representing the observations proportional to Cook's distances. Vertical reference lines are drawn at twice and three times the average hat value, horizontal reference lines at -2, 0, and 2 on the Studentized-residual scale.
```{r}
influencePlot(model = basemodel, id.n = 5)
influencePlot(model = inclusivemodel, id.n = 5)
influencePlot(model = bestmodel, id.n = 5)
```


###Global test of model assumptions
```{r}
gvmodel <- gvlma(basemodel) 
summary(gvmodel)
```

Evaluate homoscedasticity
```{r}
# non-constant error variance test
ncvTest(bestmodel)
# plot studentized residuals vs. fitted values 
spreadLevelPlot(bestmodel)
```

Evaluate Collinearity
```{r}
vif(bestmodel) # variance inflation factors 
sqrt(vif(bestmodel)) > 2 # problem?
```

Evaluate Nonlinearity
```{r}
# component + residual plot 
crPlots(bestmodel)
# Ceres plots 
ceresPlots(bestmodel)
```

Test for Autocorrelated Errors
```{r}
durbinWatsonTest(bestmodel)
```


```{r}
##Copy the dataset and remove outliers
data<-mtcars

##Set values that are lower than the 1st percentile to the 1st percentile
data$mpg[data$mpg < quantile(data$mpg,.01)] <-quantile(data$mpg,.01)

##Set values that are higher than the 99th percentile to the 99th percentile
data$mpg[data$mpg > quantile(data$mpg,.99)] <-quantile(data$mpg,.99)
```


Continue from https://rpubs.com/kaz_yos/car-residuals
http://www.statmethods.net/stats/regression.html

___
This is an [R Markdown document](http://rmarkdown.rstudio.com). Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents.