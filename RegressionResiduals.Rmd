---
title: "Regression Residuals"
author: "Dr. B"
date: "Saturday, September 27, 2014"
output: html_document
---

```{r,warning=FALSE,message=FALSE}
##Clear the environment
rm(list=ls())

##Turn off scientific notations for numbers
options(scipen = 999)  

##Set locale
Sys.setlocale("LC_ALL", "English") 

##Load libraries
call <- function(x)
{
  if (!require(x,character.only = TRUE))
        {
                install.packages(x,dep=TRUE)
        }
}
#call("MASS")

data(mtcars)
str(mtcars)
```

For more efficient analysis, I transformed transforming the following 5 variables into factors:

```{r}
mtcars$gear <- factor(mtcars$gear,levels=c(3,4,5),labels=c("3gears","4gears","5gears"))
mtcars$cyl <- factor(mtcars$cyl,levels=c(4,6,8),labels=c("4cyl","6cyl","8cyl")) 
mtcars$am <- factor(mtcars$am,levels=c(0,1),labels=c("Automatic","Manual"))
mtcars$vs <- factor(mtcars$vs)
mtcars$carb <- factor(mtcars$carb)
```

###Definitions
####Residual
The difference between the predicted value (based on the regression equation) and the actual, observed value.

####Outlier
In linear regression, an outlier is an observation with large residual. In other words, it is an observation whose dependent-variable value is unusual given its value on the predictor variables. An outlier may indicate a sample peculiarity or may indicate a data entry error or other problem.

#####Leverage
An observation with an extreme value on a predictor variable is a point with high leverage. Leverage is a measure of how far an independent variable deviates from its mean. High leverage points can have a great amount of effect on the estimate of regression coefficients.  The leverage of an observation is based on how much the observation's value on the predictor variable differs from the mean of the predictor variable. The greater an observation's leverage, the more potential it has to be an influential observation. For example, an observation with a value equal to the mean on the predictor variable has no influence on the slope of the regression line regardless of its value on the criterion variable. On the other hand, an observation that is extreme on the predictor variable has the potential to affect the slope greatly.

Calculation of Leverage (h)
The first step is to standardize the predictor variable so that it has a mean of 0 and a standard deviation of 1. Then, the leverage (h) is computed by squaring the observation's value on the standardized predictor variable, adding 1, and dividing by the number of observations.

The leverage always takes values between 0 and 1. A point with zero leverage has no effect on the regression model. If a point has leverage equal to 1 the line must follow the point perfectly. 

####Influence
An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients.  Influence can be thought of as the product of leverage and outlierness.  The influence of an observation can be thought of in terms of how much the predicted scores for other observations would differ if the observation in question were not included. 

An observation's influence is a function of two factors: (1) how much the observation's value on the predictor variable differs from the mean of the predictor variable and (2) the difference between the predicted score for the observation and its actual score. The former factor is called the observation's leverage. The latter factor is called the observation's distance.

Cook's D is a good measure of the influence of an observation and is proportional to the sum of the squared differences between predictions made with all observations in the analysis and predictions made leaving out the observation in question. If the predictions are the same with or without the observation in question, then the observation has no influence on the regression model. If the predictions differ greatly when the observation is not included in the analysis, then the observation is influential.

A common rule of thumb is that an observation with a value of Cook's D over 1.0 has too much influence. 

####Cook's distance (or Cook's D)
A measure that combines the information of leverage and residual of the observation.  The first step in calculating the value of Cook's D for an observation is to predict all the scores in the data once using a regression equation based on all the observations and once using all the observations except the observation in question. The second step is to compute the sum of the squared differences between these two sets of predictions. The final step is to divide this result by 2 times the MSE.

###Stepwise Model Selection
Performed stepwise model selection in order to select significant predictors for the final, best model. The step function will perform this selection by calling lm repeatedly to build multiple regression models and select the best variables from them using both forward selection and backward elimination methods using AIC algorithm. This  ensures that the useful variables are included in the model while omitting ones that do not contribute significantly to predicting mpg.
```{r}
bestmodel <- step(lm(mpg ~ ., data = mtcars), direction = "both")
summary(bestmodel)
residuals(bestmodel) # residuals
```

###Assessing Outliers
------------------
```{r}
require(car)
#re # Bonferonni p-value for most extreme obs
```

Model Residuals and Diagnostics
----------------------------
In this section, I have prepared the residual plots (Appendix - Figure 3) of the regression model along with computation of regression diagnostics for the linear model. This analysis was comepleted in order to examine the residuals and identify leverage points.

```{r}
residuals(bestmodel) # residuals
```

Normality of Residuals
```{r}
# qq plot for studentized resid
qqPlot(bestmodel, main="QQ Plot")
# distribution of studentized residuals
library(MASS)
sresid <- studres(bestmodel) 
hist(sresid, freq=FALSE, 
   main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40) 
yfit<-dnorm(xfit) 
lines(xfit, yfit)
```

 Evaluate homoscedasticity
```{r}
# non-constant error variance test
ncvTest(bestmodel)
# plot studentized residuals vs. fitted values 
spreadLevelPlot(bestmodel)
```

Evaluate Collinearity
```{r}
vif(bestmodel) # variance inflation factors 
sqrt(vif(bestmodel)) > 2 # problem?
```

Evaluate Nonlinearity
```{r}
# component + residual plot 
crPlots(bestmodel)
# Ceres plots 
ceresPlots(bestmodel)
```

Test for Autocorrelated Errors
```{r}
durbinWatsonTest(bestmodel)
```


An analaysis of the residual plots indicated:

1. The points in the Residuals vs. Fitted plot are randomly scattered on the plot, which verifies the condition of independence.  
2. The Normal Q-Q plot consists of the points which mostly fall on the line, which indicates that the residuals are normally distributed.  
3. The Scale-Location plot consists of points scattered in a constant band pattern, which indicates constant variance.  
4. There were some distinct points of interest (outliers or leverage points) in the top right of the plots that may indicate values of increased leverage of outliers.  

I computed regression diagnostics of the best model to identify leverage points. I computed the top three points in each case of influence measures. The data points with the most leverage in the fit are identfied by hatvalues().
```{r}
leverage <- hatvalues(bestmodel)
tail(sort(leverage),3)
```

The data points that influence the model coefficients the most are given by the dfbetas() function.
```{r}
influential <- dfbetas(bestmodel)
tail(sort(influential[,6]),3)
```

The models of vehicles identifyied above are the same models identified with the residual plots

```{r}
# Influential Observations
# added variable plots 
avPlots(bestmodel)
# Cook's D plot
# identify D values > 4/(n-k-1) 
cutoff <- 4/((nrow(mtcars)-length(bestmodel$coefficients)-2)) 
plot(bestmodel, which=4, cook.levels=cutoff)
# Influence Plot 
influencePlot(bestmodel,id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
```

###Residual Plots
---------------
```{r, echo=FALSE}
par(mfrow=c(2, 2))
plot(bestmodel)
```

```{r, echo=FALSE}
qqPlot(bestmodel, main="QQ Plot") #qq plot for studentized resid 
leveragePlots(bestmodel) # leverage plots
```

###Global test of model assumptions
```{r}
require(gvlma)
gvmodel <- gvlma(bestmodel) 
summary(gvmodel)
```