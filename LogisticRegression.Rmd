---
title: "LogisticRegression"
author: "Dr. B"
date: "Sunday, October 19, 2014"
output: html_document
---
Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables.  A logistic regression is typically used when there is one dichotomous outcome variable (such as winning or losing), and a continuous predictor variable which is related to the probability or odds of the outcome variable. It can also be used with categorical predictors, and with multiple predictors.

The goal of logistic regression is to correctly predict the category of outcome for individual cases using the most parsimonious model. To accomplish this goal, a model is created that includes all predictor variables that are useful in predicting the response variable. Several different options are available during model creation. Variables can be entered into the model in the order specified by the researcher or logistic regression can test the fit of the model after each coefficient is added or deleted, called stepwise regression.  
```{r,warning=FALSE, message=FALSE}
##Use my standard openning including call function
source('C:/Users/bryan_000/Documents/GitHub/MyWork/StdOpen.R')

##Load libraries
call("aod")
call("ggplot2")
```

###Data
A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don't admit, is a binary variable.

This dataset has a binary response (outcome, dependent) variable called admit. There are three predictor variables: gre, gpa and rank. We will treat the variables gre and gpa as continuous. The variable rank takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. We can get basic descriptives for the entire data set by using summary. To get the standard deviations, we use sapply to apply the sd function to each variable in the dataset.
```{r}
if (!exists("binary.csv")){
        # Read files
        mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
}
## Print the summary of the variables
summary(mydata)

##Get the Standard deviations of the variables
sapply(mydata, sd) 

## two-way contingency table of categorical outcome and predictors we want
## to make sure there are not 0 cells
xtabs(~admit + rank, data = mydata)
```

###Logit Regression Model
The code below estimates a logistic regression model using the glm (generalized linear model) function. First, we convert rank to a factor to indicate that rank should be treated as a categorical variable. # Note that the logit link is the default for the binomial family
```{r}
mydata$rank <- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
```

####Interpreting the Summary Output
In the output above, the first item is the call, which is the model run and the options specified.

Next is the deviance residuals, which are a measure of model fit. This part of output shows the distribution of the deviance residuals for individual cases used in the model. 

The next part of the output shows the coefficients, their standard errors, the z-statistic (sometimes called a Wald z-statistic), and the associated p-values. 

        Both gre and gpa are statistically significant, as are the three terms for rank.
        
The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.

        For every one unit change in gre, the log odds of admission (versus non-admission) increases by 0.002.
        For a one unit increase in gpa, the log odds of being admitted to graduate school increases by 0.804.
        The indicator variables for rank have a slightly different interpretation. (For example, having attended an undergraduate institution with rank of 2, versus an institution with a rank of 1, changes the log odds of admission by -0.675.) Note: rank 1, which does not show on the output, is the reference factor.

Below the table of coefficients are fit indices, including the null and deviance residuals and the AIC. 

####The Fitted Values
```{r}
fittedvalues <- fitted(mylogit)
head(fittedvalues)
plot(mylogit$fitted)
```

####Confidence intervals for the coefficient estimates
For logistic models, confidence intervals are based on the profiled log-likelihood function. We can also get CIs based on just the standard errors by using the default method.
```{r}
## CIs using profiled log-likelihood
confint(mylogit)

## CIs using standard errors
confint.default(mylogit)
```       

####Test for an Overall Effect of a Factor Variable
A test for an overall effect of a Factor variable can be done using the wald.test function of the aod library. The order in which the coefficients are given in the table of coefficients has to be the same as the order of the terms in the model. This is important because the wald.test function refers to the coefficients by their order in the model. 

A Wald test is used to test the statistical significance of each coefficient (b) in the model. A Wald test calculates a Z statistic.  This z value is then squared, yielding a Wald statistic with a chi-square distribution. However, several authors have identified problems with the use of the Wald statistic.  

When using the wald.test function. 

        b supplies the coefficients, 
        Sigma supplies the variance covariance matrix of the error terms, 
        Terms indicates terms in the model to be tested.(In this case, terms 4, 5, and 6, are the terms for the levels of rank.)
```{r}
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)
``` 

The chi-squared test statistic of 20.9, with three degrees of freedom is associated with a p-value of 0.00011 indicating that the overall effect of rank is statistically significant.

We can also test additional hypotheses about the differences in the coefficients for the different levels of a factor variable. Below we test that the coefficient for rank=2 is equal to the coefficient for rank=3. The first line of code below creates a vector l that defines the test we want to perform. In this case, we want to test the difference (subtraction) of the terms for rank=2 and rank=3 (i.e., the 4th and 5th terms in the model). 

To contrast these two terms, we multiply one of them by 1, and the other by -1. The other terms in the model are not involved in the test, so they are multiplied by 0. The second line of code below uses L=l to tell R that we wish to base the test on the vector l (rather than using the Terms option as we did above).
```{r}
l <- cbind(0, 0, 0, 1, -1, 0)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), L = l)
``` 

The chi-squared test statistic of 5.5 with 1 degree of freedom is associated with a p-value of 0.019, indicating that the difference between the coefficient for rank=2 and the coefficient for rank=3 is statistically significant.

####Odds Ratios
To get the exponentiated coefficients, you tell R that you want to exponentiate (exp), and that the object you want to exponentiate is called coefficients and it is part of mylogit (coef(mylogit)). We can use the same logic to get odds ratios and their confidence intervals, by exponentiating the confidence intervals from before. To put it all in one table, we use cbind to bind the coefficients and confidence intervals column-wise.
```{r}
## odds ratios only
exp(coef(mylogit))

## odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
``` 

Now we can say that for a one unit increase in gpa, the odds of being admitted to graduate school (versus not being admitted) increase by a factor of 2.23. Note that while R produces it, the odds ratio for the intercept is not generally interpreted.

###Inference
```{r}
anova(mylogit)

##Get test statistic
anova(mylogit)[2,2]

##Get DF
anova(mylogit)[2,1]

##Get P-value
1 - pchisq(anova(mylogit)[2,2], df=anova(mylogit)[2,1])
```

R provides a likelihood-ratio test of H0: beta_1 = 0.  
Because the P-value is very small, we reject H0, conclude beta_1 is not zero, and conclude    
that the independent variables have a significant effect on the probability of success of the dependent variable.  

###Predicted Probabilities
####At Means
Predicted probabilities can be computed for both categorical and continuous predictor variables. In order to create predicted probabilities we first need to create a new data frame with the values we want the independent variables to take on to create our predictions.
```{r}
## start by calculating the predicted probability of admission at each value of rank, holding gre and gpa at their means 
newdata1 <- with(mydata, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))

## view data frame
newdata1
```

These objects must have the same names as the variables in your logistic regression above (e.g. in this example the mean for gre must be named gre). The first line of code below is quite compact, we will break it apart to discuss what various components do. The newdata1$rankP tells R that we want to create a new variable in the dataset (data frame) newdata1 called rankP, the rest of the command tells R that the values of rankP should be predictions made using the predict( ) function. The options within the parentheses tell R that the predictions should be based on the analysis mylogit with values of the predictor variables coming from newdata1 and that the type of prediction is a predicted probability (type="response"). The second line of the code lists the values in the data frame newdata1. 
```{r}
newdata1$rankP <- predict(mylogit, newdata = newdata1, type = "response")
newdata1
```

In the above output we see that the predicted probability of being accepted into a graduate program is 0.52 for students from the highest prestige undergraduate institutions (rank=1), and 0.18 for students from the lowest ranked institutions (rank=4), holding gre and gpa at their means. 

####At Varying Numbers
We can do something very similar to create a table of predicted probabilities varying the value of gre and rank. 
```{r}
##create 100 values of gre between 200 and 800, at each value of rank (i.e., 1, 2, 3, and 4).
newdata2 <- with(mydata, data.frame(gre = rep(seq(from = 200, to = 800, length.out = 100),4), gpa = mean(gpa), rank = factor(rep(1:4, each = 100))))

##The code to generate the predicted probabilities (the first line below) also going to ask for standard errors 
##We get the estimates on the link scale
newdata3 <- cbind(newdata2, predict(mylogit, newdata = newdata2, type = "link",se = TRUE))

## We back transform both the predicted values and confidence limits into probabilities.
newdata3 <- within(newdata3, {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
})

##view some of the records
head(newdata3)
```

###Plot 
We will use the ggplot2 package for graphing. Below we make a plot with the predicted probabilities, and 95% confidence intervals.
```{r}
ggplot(newdata3, aes(x = gre, y = PredictedProb)) + geom_ribbon(aes(ymin = LL, ymax = UL, fill = rank), alpha = 0.2) + geom_line(aes(colour = rank),size = 1)
```

###Model Fit
We may also wish to see measures of how well our model fits. This can be particularly useful when comparing competing models. The output produced by summary(mylogit) included indices of fit (shown below the coefficients), including the null and deviance residuals and the AIC. 

One measure of model fit is the significance of the overall model. This test asks whether the model with predictors fits significantly better than a model with just an intercept (i.e., a null model). The null hypothesis is the deviance of the null model and the model with variables is the same.  The alternative is the deviance of the null model and the model with variables is not the same.

The test statistic is the difference between the residual deviance for the model with predictors and the null model. The test statistic is distributed chi-squared with degrees of freedom equal to the differences in degrees of freedom between the current and the null model (i.e., the number of predictor variables in the model). 
```{r}
##To find the difference in deviance for the two models (i.e., the test statistic) 
with(mylogit, null.deviance - deviance)

##The degrees of freedom for the difference between the two models is equal to the number of predictor variables in the model
with(mylogit, df.null - df.residual)

##The p-value can be obtained as follows
with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

The chi-square of 41.46 with 5 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits significantly better than an empty model.  T

```{r}
##likelihood ratio test (the deviance residual is -2*log likelihood). 
logLik(mylogit)
```

###Considerations
Empty cells or small cells: You should check for empty or small cells by doing a crosstab between categorical predictors and the outcome variable. If a cell has very few cases (a small cell), the model may become unstable or it might not run at all.

Sample size: Both logit and probit models require more cases than OLS regression because they use maximum likelihood estimation techniques. It is sometimes possible to estimate models for binary outcomes in datasets with only a small number of cases using exact logistic regression. It is also important to keep in mind that when the outcome is rare, even if the overall dataset is large, it can be difficult to estimate a logit model.

Pseudo-R-squared: Many different measures of psuedo-R-squared exist. They all attempt to provide information similar to that provided by R-squared in OLS regression; however, none of them can be interpreted exactly as R-squared in OLS regression is interpreted.

Diagnostics: The diagnostics for logistic regression are different from those for OLS regression. For a discussion of model diagnostics for logistic regression, see Hosmer and Lemeshow (2000, Chapter 5). Note that diagnostics done for logistic regression are similar to those done for probit regression.

Hosmer, D. & Lemeshow, S. (2000). Applied Logistic Regression (Second Edition). New York: John Wiley & Sons, Inc.

Long, J. Scott (1997). Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks, CA: Sage Publications.

Download Handbook AT: http://cran.r-project.org/web/packages/HSAUR/