library(vcd)
# Load functions
source('functions.R')
#Load libraries
library(epitools)
library(exact2x2)
library(e1071)
library(vcd)
library(Cmdr)
library(Rcmdr)
names.available.packages <- rownames(available.packages())
Rcmdr.related.packages <- names.available.packages[grep("Rcmdr", names.available.packages)]
Rcmdr.related.packages
install.packages(pkgs = Rcmdr.related.packages)
names.available.packages <- rownames(available.packages())
Rcmdr.related.packages <- names.available.packages[grep("Rcmdr", names.available.packages)]
Rcmdr.related.packages
install.packages(pkgs = Rcmdr.related.packages)
library(Rcmdr)
quit()
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
library(ISLR)
library(ISLR)
str(Smarket)
# Correlation minus the Direction variable
cor(Smarket[,-9])
attach(Smarket)
plot(Volume)
#  Next let’s start by fitting a logistic regression will all the variables using the glm() function, with family='binomial.
# Split data into testing and training
train<-Smarket[Year<2005,]
test<-Smarket[Year==2005,]
#Here we fit a logistic model to the stock market data using first the build in R functions, then using the caret package.
logit <- glm(Direction ~ Lag1+Lag2+Lag3, family='binomial', data=train)
summary(logit)
# Run the model on the test set
test.probs <-predict(logit, test, type='response')
#Generate random
pred.logit <- rep('Down',length(test.probs))
pred.logit[test.probs>=0.5] <- 'Up'
#Compare
table(pred.logit, test$Direction)
confusionMatrix(test$Direction, pred.logit)
modelFit<- train(Direction~Lag1+Lag2+Lag3, method='glm',preProcess=c('scale', 'center'), data=train, family=binomial(link='logit'))
summary(modelFit)
confusionMatrix(test$Direction, predict(modelFit, test))
#Odds ratio
oddsratio(x, log=FALSE)
confint(oddsratio(x, log=FALSE) )
confusionMatrix(test$Direction, predict(modelFit, test))
library(caret)
# Correlation minus the Direction variable
cor(Smarket[,-9])
attach(Smarket)
plot(Volume)
#  Next let’s start by fitting a logistic regression will all the variables using the glm() function, with family='binomial.
# Split data into testing and training
train<-Smarket[Year<2005,]
test<-Smarket[Year==2005,]
#Here we fit a logistic model to the stock market data using first the build in R functions, then using the caret package.
logit <- glm(Direction ~ Lag1+Lag2+Lag3, family='binomial', data=train)
summary(logit)
# Run the model on the test set
test.probs <-predict(logit, test, type='response')
#Generate random
pred.logit <- rep('Down',length(test.probs))
pred.logit[test.probs>=0.5] <- 'Up'
#Compare
table(pred.logit, test$Direction)
confusionMatrix(test$Direction, pred.logit)
modelFit<- train(Direction~Lag1+Lag2+Lag3, method='glm',preProcess=c('scale', 'center'), data=train, family=binomial(link='logit'))
summary(modelFit)
confusionMatrix(test$Direction, predict(modelFit, test))
# Load functions
source('functions.R')
# Load the libraries
library(e1071)
library(class)
#library(klaR)
library(caret)
#library(ElemStatLearn)
library(verification)
# Load the data
#df<-read.csv(file.choose()) #lowbwt.csv
df <-read.csv("d:/data/diabetes.csv")
#Clean the labels (names)
df<-cleanit(df)
#make Y a factor
df$diabetes <- factor(df$diabetes,levels=c(0,1), labels=c("No","Yes"))
#SPlit into train and testing datasets
sub = sample(nrow(df), floor(nrow(df) * 0.9))
train = df[sub,]
test = df[-sub,]
# break apart columns into dependent and independent variables
xTrain = train[,-9]
yTrain = train$diabetes
xTest = test[,-9]
yTest = test$diabetes
model<-train(xTrain,yTrain,'gbm',trControl=trainControl(method='cv',number=10))
modelgbm<-train(xTrain,yTrain,'gbm',trControl=trainControl(method='cv',number=10))
modelgbm
pred<-predict(modelgbm$finalModel,xTest)$class
pred<-predict(modelgbm,xTest)
predgbm<-predict(modelgbm,xTest)
tabgbm<-table(predgbm,yTest)
tabgbm
prop.table(tabgbm)
sum(tabgbm[row(tabgbm)==col(tabgbm)])/sum(tabgbm)
confusionMatrix(yTest, predgbm)
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
model<-train(xTrain,yTrain,'nb',trControl=fitControl)
model
pred<-predict(model$finalModel,xTest)$class
tabnb<-table(pred,yTest)
tabnb
confusionMatrix(yTest, pred)
myknn<- knn(xTrain,xTest,yTrain, k = 2, prob = TRUE, trControl=fitControl)
modeltree<- train(yTrain~timespregnant+plasmaglucose+diastolic+bmi+age+diabetespedigreefunction+x2hourseruminsulin + tricepsskinfoldthickness, method='rpart', data=train, tuneLength = 30,trControl=fitControl )
modeltree
summary(modeltree)
plot(modeltree)
ppredtree<-predict(modeltree, test)
confusionMatrix(yTest, predtree)
tabtree<-table(predtree,yTest)
ppredtree<-predict(modeltree, test)
predtree<-predict(modeltree, test)
tabtree<-table(predtree,yTest)
tabtree
confusionMatrix(yTest, predtree)
modelsvm<-train(xTrain,yTrain,method = "svmRadial",tuneLength = 9,trControl=fitControl)
predsvm<-predict(modelsvm, xTest)
tabsvm<-table(predsvm,yTest)
tabsvm
confusionMatrix(yTest, predsvm)
View(df)
model <- train(diabetes~, data=train, method="lvq", trControl=fitControl)
model <- train(diabetes~., data=train, method="lvq", trControl=fitControl)
model
importance <- varImp(model, scale=FALSE)
plot(importance)
modelglm<- train(yTrain~plasmaglucose+bmi+age, method='glm',data=train, family=binomial(link='logit'))
modelglm
predglm<-predict(modelglm, test, type="raw")
sum(tabglm[row(tabglm)==col(tabglm)])/sum(tabglm)
tabglm<-table(predglm,yTest)
tabglm
prop.table(tabglm)
sum(tabglm[row(tabglm)==col(tabglm)])/sum(tabglm)
confusionMatrix(yTest, predglm)
modeltree<- train(yTrain~plasmaglucose+bmi+age, method='rpart', data=train, tuneLength = 30,trControl=fitControl )
predtree<-predict(modeltree, test)
tabtree<-table(predtree,yTest)
sum(tabtree[row(tabtree)==col(tabtree)])/sum(tabtree)
confusionMatrix(yTest, predtree)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(train[,1:8], train[,9], sizes=c(1:8), rfeControl=control)
print(results)
predictors(results)
plot(results, type=c("g", "o"))
print(results)
results <- rfe(xTrain, yTrain, sizes=c(1:8), rfeControl=control)
print(results)
predictors(results)
predictors(modelrf)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
modelrf <- rfe(xTrain, yTrain, sizes=c(1:8), rfeControl=control)
# summarize the results
print(modelrf)
# list the chosen features
predictors(modelrf)
# plot the results
plot(modelrf, type=c("g", "o"))
predict(modelrf,xTest)
confusionMatrix(yTest, predict(modelrf,xTest))
predrf<-predict(modelrf,xTest)
confusionMatrix(yTest, predrf)
predrf
predgbm
predrf[1,]
predrf[,1]
confusionMatrix(yTest, predrf[,1])
print(modelrf$finalModel)
rf_model<-train(diabetes~.,data=train,method="rf",
trControl=fitControl,
prox=TRUE,allowParallel=TRUE)
modelrf<-rf_model<-train(diabetes~.,data=train,method="rf",trControl=fitControl,prox=TRUE,allowParallel=TRUE)
modelrf
summary(modelrf)
predrf<-predict(modelrf, test)
tabrf<-table(predrf,yTest)
tabrf
prop.table(tabrf)
sum(tabrf[row(tabrf)==col(tabrf)])/sum(tabrf)
confusionMatrix(yTest, predrf)
require(caret)
require(ggplot2)
require(randomForest)
training_URL<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_URL<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training<-read.csv(training_URL,na.strings=c("NA",""))
test<-read.csv(test_URL,na.strings=c("NA",""))
View(training)
testing<-read.csv(test_URL,na.strings=c("NA",""))
training<-training[,7:160]
test<-test[,7:160]
training<-training[,7:160]
testing<-testing[,7:160]
testing<-read.csv(test_URL,na.strings=c("NA",""))
testing<-testing[,7:160]
View(training)
colSums(!is.na(df))
colSums(!is.na(training))
dim(training)
apply(!is.na(training),2,sum)>19621
mostly_data<-apply(!is.na(training),2,sum)>19621
training<-training[,mostly_data]
testing<-testing[,mostly_data]
fitControl <- trainControl(##10-fold CV
method = "repeatedcv",
number = 10,
## repeated three times
repeats = 3)
rf_model<-train(classe~.,data=training,method="rf",trControl=fitControl,prox=TRUE,allowParallel=TRUE)
warnings()
InTrain<-createDataPartition(y=training$classe,p=0.1,list=FALSE)
training1<-training[InTrain,]
View(training1)
rf_model<-train(classe~.,data=training1,method="rf",trControl=fitControl,prox=TRUE,allowParallel=TRUE)
fitControl <- trainControl(method = "repeatedcv",number =2,repeats = 1)
rf_model<-train(classe~.,data=training1,method="rf",trControl=fitControl,prox=TRUE,allowParallel=TRUE)
print(rf_model)
print(rf_model$finalModel)
pred<-predict(rf_model$finalModel,testing)$classe
pred<-predict(rf_model$finalModel,testing)
pred
confusionMatrix(testing$classe, pred)
summary(pred)
print(rf_model)
print(rf_model$finalModel)
plot(rf_model)
plot(rf_model$finalModel)
str(rf_model)
rf_model$prediction
rf_model$modelinfo
varImp(rf_model$finalModel)
varImp(rf_model$finalModel, top20)
varImp(rf_model$finalModel, top=20)
varImp(rf_model$finalModel, top=5)
?varImp
?arImp.randomForest
?varImp.randomForest
plot(varImp(rf_model$finalModel), top = 20)
plot(varImp(rf_model$finalModel), top = 5)
warnings()
varp<- varImp(rf_model$finalModel)
plot(varp, top = 5)
varp<- varImp(rf_model$finalModel, scale=FALSE)
varp<- varImp(rf_model$finalModel, scale=FALSE)
plot(varp, top = 5)
plot(varp)
plot(varp)
varImp(rf_model, scale=FALSE)
plot(varImp(rf_model, scale=FALSE),top=5)
plot(varImp(rf_model, scale=FALSE),top=10)
plot(varImp(rf_model),top=10)
pred<-predict(rf_model$finalModel,testing)
summary(pred)
# Clear the environment
rm(list=ls())
# Turn off scientific notations for numbers
options(scipen = 999)
# Set locale
Sys.setlocale("LC_ALL", "English")
# Set seed for reproducibility
set.seed(2345)
# Load the libraries
library(psych)
library(e1071)
library(caret)
library(fBasics)
# Load the data
#Load the birthweight data (lowbw.csv)
df<-read.csv(file.choose())
# Load the data
#Load the birthweight data (lowbw.csv)
df<-read.csv(file.choose())
colSums(!is.na(df))
names(df) <-tolower(names(df))
names(df) <- gsub("\\(","",names(df))
names(df) <- gsub("\\)","",names(df))
names(df) <- gsub("\\.","",names(df))
names(df) <- gsub("_","",names(df))
names(df) <- gsub("-","",names(df))
names(df) <- gsub(",","",names(df))
df$id <-NULL
df$bwt <-NULL
# Create factor
df$race <- factor(df$race, levels=c(1,2,3),labels=c("white","black","other"))
df$smoke<- factor(df$smoke, levels=c(0,1),labels=c("nonsmoker","smoker"))
df$ht<- factor(df$ht, levels=c(0,1),labels=c("noHT","yesHT"))
df$ui<- factor(df$ui, levels=c(0,1),labels=c("noUI","yesUI"))
df$ptl<- factor(df$ptl, levels=c(0,1),labels=c("noPM","yesPML"))
depvar <- 'low'
indepvars <-c('lwt','race','age')
xtabs(~get(depvar) + get(indepvar), data = df)
indepvar <- 'race'
indepvars <-c('lwt','race')
xtabs(~get(depvar) + get(indepvar), data = df)
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
fit<-step(glm(f1,data=df,family=binomial),direction="both")
summary(fit) # display results
exp(cbind(OR = coef(fit), confint(fit))) ## odds ratios and 95% CI together
logLik(fit)
test<-dataframe(lwt=100,race="black")
test<-data.frame(lwt=100,race="black")
predict(fit,test)
confint(predict(fit,test))
confint(predict(fit,test))
predict(fit,test)
confint.default(predict(fit,test))
fitpred<-predict(fit,test)
confint(fitpred)
fitpred<-predict(fit,test,type = "link", se.fit = TRUE)
fitpred<-predict(fit,test,type = "link", se.fit = TRUE)
fitpred
fitpred<-predict(fit,test,interval="predict", se.fit = TRUE)
summary(fitpred)
fitpred
fitpred<-predict(fit,test,interval="predict")
fitpred
fit
fitpred<-predict(fit,test,type = "response")
confint(fitpred)
fitpred
fit<-step(glm(f1,data=df,family=binomial),direction="both")r
fit<-step(glm(f1,data=df,family=binomial),direction="both")
exp(cbind(OR = coef(fit), confint(fit))) ## odds ratios and 95% CI together
d<-anova(fit,test='Chisq') # or d<-anova(fit,test='LRT')
d
test<-data.frame(lwt=100,race="black")
fitpred<-predict(fit,test,type = "response")
fitpred
confint(fitpred)
adjust(fitpred)
?predict
fitpred<-predict(fit,test,type = "response",se,fit=TRUE)
fitpred<-predict(fit,test,type = "response",se.fit=TRUE)
fitpred
fitpred$se.fit
fitpred$se.fit*1.96
fitpred$fit - fitpred$se.fit*1.96
cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
fitpred<-predict(fit,test,se.fit=TRUE)
fitpred<-predict(fit,test,se.fit=TRUE)
cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi <- cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
exp(pi)
fitpred2<-predict(fit,test,type=response, se.fit=TRUE)
fitpred2<-predict(fit,test,type="response", se.fit=TRUE)
fitpred2
pi2 <- cbind(Prob=fitpred2$fit,LCL=fitpred2$fit - fitpred2$se.fit*1.96,UCL=fitpred2$fit + fitpred2$se.fit*1.96)
pi2
pi2 <- cbind(Prob=exp(pred$fit/(1+pred$fit),LCL=fitpred2$fit - fitpred2$se.fit*1.96,UCL=fitpred2$fit + fitpred2$se.fit*1.96)
)
fitpred2<-predict(fit,test,type="response", se.fit=TRUE)
pi2 <- cbind(Prob=exp(pred$fit/(1+pred$fit)),LCL=fitpred2$fit - fitpred2$se.fit*1.96,UCL=fitpred2$fit + fitpred2$se.fit*1.96)
pi2 <- cbind(Prob=exp(fitpred$fit/(1+fitpred$fit)),LCL=fitpred2$fit - fitpred2$se.fit*1.96,UCL=fitpred2$fit + fitpred2$se.fit*1.96)
pi2
pi2 <- cbind(Prob=log(fitpred$fit/(1+fitpred$fit)),LCL=fitpred2$fit - fitpred2$se.fit*1.96,UCL=fitpred2$fit + fitpred2$se.fit*1.96)
pi2
log(pi)
exp(pi)
pi
d<-anova(fit,test='Chisq') # or d<-anova(fit,test='LRT')
d
pi2 <- cbind(Prob=(log(fitpred$fit)/(1+log(fitpred$fit)),LCL=fitpred2$fit - fitpred2$se.fit*1.96,UCL=fitpred2$fit + fitpred2$se.fit*1.96)
log(fitpred$fit)/(1+log(fitpred$fit))
fitpred$fit
log(fitpred$fit)/(1+log(fitpred$fit))
log(fitpred$fit)
exp(fitpred$fit)
exp(fitpred$fit)/(1+exp(fitpred$fit))
pi
pi$prob
pi[,1]
exp(pi)
pi[,1]
pi2 <- cbind(Prob=exp(pi[,1]),LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2
pi2 <- cbind(Prob=exp(pi[,1]/(1+exp(pi[,1]))),LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2
exp(pi[,1])/(1+exp(pi[,1]))
pi2 <- cbind(Prob=xp(pi[,1])/(1+exp(pi[,1])),LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=xp(pi[,2])/(1+exp(pi[,2])),UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=exp(pi[,2])/(1+exp(pi[,2])),UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2
pi <- cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=exp(pi[,2])/(1+exp(pi[,2])),UCL=exp(pi[,1])/(1+exp(pi[,1])))
#Note 59% chance of naving a low birthweight baby
pi
pi2
test<-data.frame(lwt=100,race="black")
fitpred<-predict(fit,test,se.fit=TRUE)
pi <- cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=exp(pi[,2])/(1+exp(pi[,2])),UCL=exp(pi[,3])/(1+exp(pi[,3])))
#Note 59% chance of naving a low birthweight baby
pi
pi2
test<-data.frame(lwt=100,race="white")
fitpred<-predict(fit,test,se.fit=TRUE)
pi <- cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=exp(pi[,2])/(1+exp(pi[,2])),UCL=exp(pi[,3])/(1+exp(pi[,3])))
#Note 59% chance of having a low birthweight baby for a black woman with lwt=100
pi
pi2
# Make a prediction for a black woman with lwt=100
test<-data.frame(lwt=100,race="other")
fitpred<-predict(fit,test,se.fit=TRUE)
pi <- cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=exp(pi[,2])/(1+exp(pi[,2])),UCL=exp(pi[,3])/(1+exp(pi[,3])))
#Note 59% chance of having a low birthweight baby for a black woman with lwt=100
pi
pi2
get LR
d$Deviance
# Get loglikelihood
logLik(fit)
# Make a prediction for a black woman with lwt=100
test<-data.frame(lwt=100,race="black")
fitpred<-predict(fit,test,se.fit=TRUE)
pi <- cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=exp(pi[,2])/(1+exp(pi[,2])),UCL=exp(pi[,3])/(1+exp(pi[,3])))
#Note 59% chance of having a low birthweight baby for a black woman with lwt=100
pi
pi2
pi2*100
pi2
df<-read.csv(lowbw.csv)
df<-read.csv("lowbw.csv")
df<-read.csv("lowbwt.csv")
df<-read.csv("D:/Data/lowbwt.csv")
# Load functions
source('functions.R')
# Load the libraries
library(psych)
library(e1071)
library(caret)
library(fBasics)
# Load the data
#Load the birthweight data (lowbw.csv)
#df<-read.csv(file.choose())
df<-read.csv("D:/Data/lowbwt.csv")
# count blanks remove blanks
colSums(!is.na(df))
#df <- na.omit(df)
#colSums(!is.na(df))
# Clean
df<- cleanit(df)
# remove a column
df$id <-NULL
df$bwt <-NULL
# Create dummies
df$white <- as.numeric(df$race == 1)
df$black <- as.numeric(df$race == 2)
df$other <- as.numeric(df$race == 3)
df$race <- factor(df$race, levels=c(1,2,3),labels=c("white","black","other"))
df$smoke<- factor(df$smoke, levels=c(0,1),labels=c("nonsmoker","smoker"))
df$ht<- factor(df$ht, levels=c(0,1),labels=c("noHT","yesHT"))
df$ui<- factor(df$ui, levels=c(0,1),labels=c("noUI","yesUI"))
df$ptl<- factor(df$ptl, levels=c(0,1),labels=c("noPM","yesPML"))
#De
depvar <- 'low'
indepvar <- 'race'
indepvars <-c('lwt','race')
# two-way contingency table of categorical outcome and predictors we want
#  to make sure there are not 0 cells
xtabs(~get(depvar) + get(indepvar), data = df)
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
fit<-step(glm(f1,data=df,family=binomial),direction="both")
summary(fit) # display results
confint(fit) # 95% CI for the coefficients using profiled log-likelihood
confint.default(fit) # 95% CI for the coefficients using standard errors
#exp(coef(fit)) # exponentiated coefficients a.k.a odds ratios
#exp(confint(fit)) # 95% CI for exponentiated coefficients
exp(cbind(OR = coef(fit), confint(fit))) ## odds ratios and 95% CI together
d<-anova(fit,test='Chisq') # or d<-anova(fit,test='LRT')
d
# get LR
d$Deviance
# Get loglikelihood
logLik(fit)
# Make a prediction for a black woman with lwt=100
test<-data.frame(lwt=100,race="black")
fitpred<-predict(fit,test,se.fit=TRUE)
pi <- cbind(Prob=fitpred$fit,LCL=fitpred$fit - fitpred$se.fit*1.96,UCL=fitpred$fit + fitpred$se.fit*1.96)
pi2 <- cbind(Prob=exp(pi[,1])/(1+exp(pi[,1])),LCL=exp(pi[,2])/(1+exp(pi[,2])),UCL=exp(pi[,3])/(1+exp(pi[,3])))
#Note 59% chance of having a low birthweight baby for a black woman with lwt=100
pi
pi2
