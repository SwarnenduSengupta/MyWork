jripFit1 <- train(TrainData, TrainClasses,method = "JRip")
jripFit1
plot(jripFit1)
#Second Model
jripFit2 <- train(TrainData, TrainClasses,method = "JRip",preProcess = c("center", "scale"),tuneLength = 10,trControl = trainControl(method = "cv"))
jripFit2
plot(jripFit2)
# K means
neighborCount=2
modelKNN <- knn3(Species ~ ., data = train, k = neighborCount, prob = TRUE)
predKNN <- predict(modelKNN, testInd, type = "prob")
confKNN <- table(predKNN)
rm(list=ls())
library(caret)
library(RWeka)
set.seed(1234)
TrainData <- iris[,1:4]
TrainClasses <- iris[,5]
# separate data into test and train sets, 70/30 split in this case
splitIndex <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train <- iris[splitIndex, ]
test <- iris[-splitIndex, ]
testInd <- test[ ,!colnames(test) %in% "Species"]
testDep <- as.factor(test[, names(test) == "Species"])
rm(list=ls())
library(caret)
library(RWeka)
set.seed(1234)
# separate data into test and train sets, 70/30 split in this case
splitIndex <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train <- iris[splitIndex, ]
test <- iris[-splitIndex, ]
testInd <- test[ ,!colnames(test) %in% "Species"]
testDep <- as.factor(test[, names(test) == "Species"])
TrainData <- iris[,1:4]
TrainClasses <- iris[,5]
#First Model
jripFit1 <- train(TrainData, TrainClasses,method = "JRip")
jripFit1
plot(jripFit1)
#Second Model
jripFit2 <- train(TrainData, TrainClasses,method = "JRip",preProcess = c("center", "scale"),tuneLength = 10,trControl = trainControl(method = "cv"))
jripFit2
plot(jripFit2)
# K means
neighborCount=2
modelKNN <- knn3(Species ~ ., data = train, k = neighborCount, prob = TRUE)
predKNN <- predict(modelKNN, testInd, type = "prob")
confKNN <- table(predKNN)
confKNN <- table(predKNN)
table(predKNN)
confKNN <- confusionMatrix(testDep, predKNN)
km <- kmeans(iris[,1:4], 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, iris$Species)
preddKNN
predKNN
km2 <- kmeans(iris[,1:4], 3)
plot(iris[,1], iris[,2], col=km2$cluster)
points(km2$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km2$cluster, iris$Species)
table(km$cluster, iris$Species)
m <- matrix(1:15,5,3)
dist(m) # computes the distance between rows of m (since there are 3 columns, it is the euclidian distance between tri-dimensional points)
dist(m,method="manhattan") # using the manhattan metric
sampleiris <- iris[sample(1:150, 40),] # get samples from iris dataset
# each observation has 4 variables, ie, they are interpreted as 4-D points
distance   <- dist(sampleiris[,-5], method="euclidean")
cluster    <- hclust(distance, method="average")
plot(cluster, hang=-1, label=sampleiris$Species)
plot(as.dendrogram(cluster), edgePar=list(col="darkgreen", lwd=2), horiz=T)
str(as.dendrogram(cluster)) # Prints dendrogram structure as text.
cluster$labels[cluster$order] # Prints the row labels in the order they appear in the tree.
#Prune by cluster
par(mfrow=c(1,2))
group.3 <- cutree(cluster, k = 3)  # prune the tree by 3 clusters
table(group.3, sampleiris$Species) # compare with known classes
plot(sampleiris[,c(1,2)], col=group.3, pch=19, cex=2.5, main="3 clusters")
points(sampleiris[,c(1,2)], col=sampleiris$Species, pch=19, cex=1)
group.6 <- cutree(cluster, k = 6)  # we can prune by more clusters
table(group.6, sampleiris$Species)
plot(sampleiris[,c(1,2)], col=group.6, pch=19, cex=2.5, main="6 clusters")
points(sampleiris[,c(1,2)], col=sampleiris$Species, pch=19, cex=1) # the little points are the true classes
par(mfrow=c(1,1))
plot(cluster, hang=-1, label=sampleiris$Species)
abline(h=0.9,lty=3,col="red")
height.0.9 <- cutree(cluster, h = 0.9)
table(height.0.9, sampleiris$Species) # compare with known classes
plot(sampleiris[,c(1,2)], col=height.0.9, pch=19, cex=2.5, main="3 clusters")
points(sampleiris[,c(1,2)], col=sampleiris$Species, pch=19, cex=1)
# Calculate the dissimilarity between observations using the Euclidean distance
dist.iris <- dist(iris, method="euclidean")
# Compute a hierarchical cluster analysis on the distance matrix using the complete linkage method
h.iris <- hclust(dist.iris, method="complete")
h.iris
head(h.iris$merge, n=10)
plot(h.iris)
h.iris.heights <- h.iris$height # height values
h.iris.heights[1:10]
subs <- round(h.iris.heights - c(0,h.iris.heights[-length(h.iris.heights)]), 3) # subtract next height
which.max(subs)
plot(cluster); rect.hclust(cluster, k=5, border="red")
plot(cluster); rect.hclust(cluster, k=3, border="red")
plot(cluster); rect.hclust(cluster, k=2, border="red")
plot(cluster); rect.hclust(cluster, k=1, border="red")
2
plot(cluster); rect.hclust(cluster, k=22, border="red")
plot(cluster); rect.hclust(cluster, k=6, border="red")
rm(list=ls())
library(caret)
set.seed(1234)
# separate data into test and train sets, 70/30 split in this case
splitIndex <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train <- iris[splitIndex, ]
test <- iris[-splitIndex, ]
testInd <- test[ ,!colnames(test) %in% "Species"]
testDep <- as.factor(test[, names(test) == "Species"])
TrainData <- iris[,1:4]
TrainClasses <- iris[,5]
km <- kmeans(iris[,1:4], 3)
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, iris$Species)
km <- kmeans(test, 3)
km <- kmeans(test)
km <- kmeans(test[,1:4], 3)
plot(test[,1], test[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, test$Species)
km2 <- kmeans(test[,1:4], 3)
plot(test[,1], test[,2], col=km2$cluster)
points(km2$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km2$cluster, test$Species)
```
km <- kmeans(test[,1:4], 3)
plot(test[,1], test[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km$cluster, test$Species)
table(km2$cluster, km1$cluster)
table(km2$cluster, km$cluster)
#First Round
km1 <- kmeans(test[,1:4], 3)
plot(test[,1], test[,2], col=km1$cluster)
points(km1$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km1$cluster, test$Species)
km2 <- kmeans(test[,1:4], 3)
plot(test[,1], test[,2], col=km2$cluster)
points(km2$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km2$cluster, test$Species)
table(km1$cluster, km2$cluster)
table(km2$cluster, km1$cluster)
table(km1$cluster, km2$cluster)
m <- matrix(1:15,5,3)
dist(m) # computes the distance between rows of m (since there are 3 columns, it is the euclidian distance between tri-dimensional points)
dist(m,method="manhattan") # using the manhattan metric
distance   <- dist(test[,-5], method="euclidean")
cluster    <- hclust(distance, method="average")
distance   <- dist(train[,-5], method="euclidean")
cluster    <- hclust(distance, method="average")
plot(cluster, hang=-1, label=train$Species)
str(as.dendrogram(cluster)) # Prints dendrogram structure as text.
cluster$labels[cluster$order] # Prints the row labels in the order they appear in the tree.
plot(as.dendrogram(cluster), edgePar=list(col="darkgreen", lwd=2), horiz=T)
par(mfrow=c(1,2))
group.3 <- cutree(cluster, k = 3)  # prune the tree by 3 clusters
table(group.3, train$Species) # compare with known classes
plot(train[,c(1,2)], col=group.3, pch=19, cex=2.5, main="3 clusters")
points(train[,c(1,2)], col=train$Species, pch=19, cex=1)
group.6 <- cutree(cluster, k = 6)  # we can prune by more clusters
table(group.6, train$Species)
plot(train[,c(1,2)], col=group.6, pch=19, cex=2.5, main="6 clusters")
points(train[,c(1,2)], col=sampleiris$Species, pch=19, cex=1) # the little points are the true classes
par(mfrow=c(1,1))
plot(cluster, hang=-1, label=train$Species)
abline(h=0.9,lty=3,col="red")
height.0.9 <- cutree(cluster, h = 0.9)
table(height.0.9, train$Species) # compare with known classes
par(mfrow=c(1,1))
plot(cluster, hang=-1, label=train$Species)
abline(h=0.9,lty=3,col="red")
height.0.9 <- cutree(cluster, h = 0.9)
table(height.0.9, train$Species) # compare with known classes
plot(train[,c(1,2)], col=height.0.9, pch=19, cex=2.5, main="3 clusters")
points(train[,c(1,2)], col=train$Species, pch=19, cex=1)
# Calculate the dissimilarity between observations using the Euclidean distance
dist.iris <- dist(train, method="euclidean")
# Compute a hierarchical cluster analysis on the distance matrix using the complete linkage method
h.iris <- hclust(dist.iris, method="complete")
h.iris
head(h.iris$merge, n=10)
plot(h.iris)
h.iris.heights <- h.iris$height # height values
h.iris.heights[1:10]
subs <- round(h.iris.heights - c(0,h.iris.heights[-length(h.iris.heights)]), 3) # subtract next height
which.max(subs)
# Cuts dendrogram at specified level and draws rectangles around the resulting clusters
plot(cluster); rect.hclust(cluster, k=6, border="red")
# Clear the environment
rm(list=ls())
# Turn off scientific notations for numbers
options(scipen = 999)
# Set locale
Sys.setlocale("LC_ALL", "English")
# Set seed for reproducibility
set.seed(2345)
# Load the libraries
library(caret)
# separate data into test and train sets, 70/30 split in this case
splitIndex <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train <- iris[splitIndex, ]
test <- iris[-splitIndex, ]
testInd <- test[ ,!colnames(test) %in% "Species"]
testDep <- as.factor(test[, names(test) == "Species"])
TrainData <- iris[,1:4]
TrainClasses <- iris[,5]
View(splitIndex)
km1 <- kmeans(test[,1:4], 3)
plot(test[,1], test[,2], col=km1$cluster)
points(km1$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km1$cluster, test$Species)
km2 <- kmeans(test[,1:4], 3)
plot(test[,1], test[,2], col=km2$cluster)
points(km2$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km2$cluster, test$Species)
View(testInd)
View(test)
View(testInd)
testInd <- test[ ,!colnames(test) %in% "Species"]
testDep <- as.factor(test[, names(test) == "Species"])
trainInd <- test[ ,!colnames(train) %in% "Species"]
TrainDep <- as.factor(test[, names(test) == "Species"])
testInd <- test[ ,!colnames(test) %in% "Species"]
testDep <- as.factor(test[, names(test) == "Species"])
trainInd <- test[ ,!colnames(train) %in% "Species"]
TrainDep <- as.factor(train[, names(train) == "Species"])
testInd <- test[ ,!colnames(test) %in% "Species"]
testDep <- as.factor(test[, names(test) == "Species"])
trainInd <- train[ ,!colnames(train) %in% "Species"]
TrainDep <- as.factor(train[, names(train) == "Species"])
```
testInd <- test[ ,!colnames(test) %in% "Species"]
testDep <- as.factor(test[, names(test) == "Species"])
trainInd <- train[ ,!colnames(train) %in% "Species"]
trainDep <- as.factor(train[, names(train) == "Species"])
# Remove unsed
rm(train)
rm(test)
rm(splitIndex)
km1 <- kmeans(testInd, 3)
plot(test[,1], test[,2], col=km1$cluster)
plot(testInd[,1], testInd[,2], col=km1$cluster)
points(km1$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km1$cluster, testDep)
# First Round
km1 <- kmeans(trainInd, 3)
plot(trainInd[,1], trainInd[,2], col=km1$cluster)
points(km1$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km1$cluster, trainDep)
table(km1$cluster, testDep)
km1 <- kmeans(trainInd, 3)
plot(trainInd[,1], trainInd[,2], col=km1$cluster)
points(km1$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km1$cluster, trainDep)
km2 <- kmeans(train, 3)
plot(train[,1], train[,2], col=km2$cluster)
points(km2$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km2$cluster, trainDep)
# Another ROund
km2 <- kmeans(trainInd, 3)
plot(trainInd[,1], trainInd[,2], col=km2$cluster)
points(km2$centers[,c(1,2)], col=1:3, pch=19, cex=2)
table(km2$cluster, trainDep)
table(km1$cluster, km2$cluster)
m <- matrix(1:15,5,3)
dist(m) # computes the distance between rows of m (since there are 3 columns, it is the euclidian distance between tri-dimensional points)
dist(m,method="manhattan") # using the manhattan metric
distance   <- dist(train[,-5], method="euclidean")
cluster    <- hclust(distance, method="average")
plot(cluster, hang=-1, label=train$Species)
plot(as.dendrogram(cluster), edgePar=list(col="darkgreen", lwd=2), horiz=T)
str(as.dendrogram(cluster)) # Prints dendrogram structure as text.
cluster$labels[cluster$order] # Prints the row labels in the order they appear in the tree.
distance   <- dist(trainInd, method="euclidean")
cluster    <- hclust(distance, method="average")
plot(cluster, hang=-1, label=trainDep)
# each observation has 4 variables, ie, they are interpreted as 4-D points
distance   <- dist(trainInd, method="euclidean")
cluster    <- hclust(distance, method="average")
plot(cluster, hang=-1, label=trainDep)
plot(as.dendrogram(cluster), edgePar=list(col="darkgreen", lwd=2), horiz=T)
str(as.dendrogram(cluster)) # Prints dendrogram structure as text.
cluster$labels[cluster$order] # Prints the row labels in the order they appear in the tree
group.3 <- cutree(cluster, k = 3)  # prune the tree by 3 clusters
table(group.3, train$Species) # compare with known classes
plot(trainInd[,c(1,2)], col=group.3, pch=19, cex=2.5, main="3 clusters")
points(trainInd[,c(1,2)], col=trainDep, pch=19, cex=1)
group.6 <- cutree(cluster, k = 6)  # we can prune by more clusters
table(group.6, train$Species)
plot(trainInd[,c(1,2)], col=group.6, pch=19, cex=2.5, main="6 clusters")
points(trainInd[,c(1,2)], col=trainDep, pch=19, cex=1) # the little points are the true classes
#Prune by cluster
par(mfrow=c(1,2))
group.3 <- cutree(cluster, k = 3)  # prune the tree by 3 clusters
table(group.3, trainDep) # compare with known classes
plot(trainInd[,c(1,2)], col=group.3, pch=19, cex=2.5, main="3 clusters")
points(trainInd[,c(1,2)], col=trainDep, pch=19, cex=1)
group.6 <- cutree(cluster, k = 6)  # we can prune by more clusters
table(group.6, trainDep)
plot(trainInd[,c(1,2)], col=group.6, pch=19, cex=2.5, main="6 clusters")
points(trainInd[,c(1,2)], col=trainDep, pch=19, cex=1) # the little points are the true classes
par(mfrow=c(1,1))
plot(cluster, hang=-1, label=trainDep)
abline(h=0.9,lty=3,col="red")
height.0.9 <- cutree(cluster, h = 0.9)
table(height.0.9, trainDep) # compare with known classes
par(mfrow=c(1,1))
plot(cluster, hang=-1, label=trainDep)
abline(h=0.9,lty=3,col="red")
height.0.9 <- cutree(cluster, h = 0.9)
table(height.0.9, trainDep) # compare with known classes
plot(trainInd[,c(1,2)], col=height.0.9, pch=19, cex=2.5, main="3 clusters")
points(trainInd[,c(1,2)], col=TrainDep, pch=19, cex=1)
# Calculate the dissimilarity between observations using the Euclidean distance
dist.iris <- dist(trainInd, method="euclidean")
# Compute a hierarchical cluster analysis on the distance matrix using the complete linkage method
h.iris <- hclust(dist.iris, method="complete")
h.iris
head(h.iris$merge, n=10)
plot(h.iris)
h.iris.heights <- h.iris$height # height values
h.iris.heights[1:10]
subs <- round(h.iris.heights - c(0,h.iris.heights[-length(h.iris.heights)]), 3) # subtract next height
which.max(subs)
# Cuts dendrogram at specified level and draws rectangles around the resulting clusters
plot(cluster); rect.hclust(cluster, k=6, border="red")
plot(cluster); rect.hclust(cluster, k=r, border="red")
plot(cluster); rect.hclust(cluster, k=3, border="red")
plot(trainInd[,c(1,2)], col=height.0.9, pch=19, cex=2.5, main="3 clusters")
points(trainInd[,c(1,2)], col=trainDep, pch=19, cex=1)
# Calculate the dissimilarity between observations using the Euclidean distance
dist.iris <- dist(trainInd, method="euclidean")
# Compute a hierarchical cluster analysis on the distance matrix using the complete linkage method
h.iris <- hclust(dist.iris, method="complete")
h.iris
head(h.iris$merge, n=10)
plot(h.iris)
h.iris.heights <- h.iris$height # height values
h.iris.heights[1:10]
subs <- round(h.iris.heights - c(0,h.iris.heights[-length(h.iris.heights)]), 3) # subtract next height
which.max(subs)
# Cuts dendrogram at specified level and draws rectangles around the resulting clusters
plot(cluster); rect.hclust(cluster, k=3, border="red")
Aryan
Name<- c('Aryan', 'Gopal','Zubin','Ravi','Umesh','Anita')
Gender<-c('M','M','F','M','M','F')
Age<=c(20,21,24,26,26,23)
Age<-c(20,21,24,26,26,23)
Income<-c(20,30,35,40,41,50)
Records<-data.frame(name,Gender,Age,Income)
Records<-data.frame(Name,Gender,Age,Income)
Str(Records)
strRecords)
str(Records)
summary(Records)
attach(Records)
plot(Age)
plot(Income)
rm(Name)
rm(Gender)
rm(Age)
rm(Income)
plot(age)
plot(Age)
plot(income)
plot(Income)
plot(Gender~Income)
plot(Gender)
plot(Age~Income)
plot(Income~Age)
plot(Income~Gender)
plot(Income~Age)
# Clear the environment
rm(list=ls())
# Turn off scientific notations for numbers
options(scipen = 999)
# Set locale
Sys.setlocale("LC_ALL", "English")
# Set seed for reproducibility
set.seed(2345)
# Create the Vectors
Name<- c('Aryan', 'Gopal','Zubin','Ravi','Umesh','Anita')
Gender<-c('M','M','F','M','M','F')
Age<-c(20,21,24,26,26,23)
Income<-c(20,30,35,40,41,50)
# Create the Data Frame
Records<-data.frame(Name,Gender,Age,Income)
# Attach it
attach(Records)
# Remove the vectors
rm(Name)
rm(Gender)
rm(Age)
rm(Income)
#Look at Records
str(Records)
summary(Records)
# Make some Plots
plot(Age)
plot(Income)
plot(Gender)
plot(Income~Age)
plot(Income~Gender)
#Look at Records
str(Records)
summary(Records)
# Make some Plots
plot(Age)
plot(Income)
plot(Gender)
plot(Income~Age)
plot(Income~Gender)
abline(mean(Income),color='red' )
abline(mean(Income),col='red' )
?abline
abline(h=mean(Income),col='red' )
plot(Age)
plot(Income)
abline(h=mean(Income),col='red' )
plot(Age)
plot(Age)
abline(h=mean(Age),col='red' )
plot(Gender)
plot(Income~Age)
abline(v=mean(Age),col='red' )
abline(v=mean(Age),(h=mean(Income), col='red' )
abline(v=mean(Age),h=mean(Income), col='red' )
abline(v=mean(Age),col='red',h=mean(Income), col='blue' )
abline(v=mean(Age),col='red',h=mean(Income), col='blue' )
abline(v=mean(Age),col='red',h=mean(Income) )
plot(Income~Gender)
boxplot(Income~Age)
Records[Records$Age<23]
Records[Records$Age<23,]
data1<- Records[Records$Age<23,]
data1
nrows(data1)
nrow(data1)
data2<- Records[Records$Gender=="M"&Records$Age>21,]
data2
nrow(data2)
Records$Gender_dummy<- ifelse(Records$Gender=="M",1,0) ##Assigns 1 to males 0 to females
View(Records)
# Clear the environment
rm(list=ls())
# Turn off scientific notations for numbers
options(scipen = 999)
# Set locale
Sys.setlocale("LC_ALL", "English")
# Set seed for reproducibility
set.seed(2345)
# Create the Dataframes for merger
x<- data.frame(k1=c(1,2,3),k2=c('a','b','c'),data =1:3)
y<- data.frame(k1=c(1,7,8,9,10),k3=c(5,6,7,8,9))
merge (x,y,by.x="k1",by.y="k1",all=TRUE)
merge (x,y,by.x="k1",by.y="k1")
merge (x,y,by.x="k1",by.y="k1",all.x=TRUE)
merge (x,y,by.x="k1",by.y="k1",all.y=TRUE)
unif(1:100)
?uniform
lamba
rnorm
rnorm(1:100)
runif(1:100)
x<-runif(1:100)
dist(x)
hist(x)
x<-runif(1,100,100)
hist(x)
x
x<-runif(100,1,100)
x
hist(x)
x<-runif(100,1,100)
x [seq(from=10,by=10, length=10)]
x
x <- runif(100,1,100)
x
# Clear the environment
rm(list=ls())
# Turn off scientific notations for numbers
options(scipen = 999)
# Set locale
Sys.setlocale("LC_ALL", "English")
# Set seed for reproducibility
set.seed(2345)
# generate 100 random numbers from a uniform distribution from 1 to 100
x <- runif(100,1,100)
x
x [seq(from=10,by=10,length=10)]
?runif
?rnorm
y <- rnorm(100,0,1)
y
hist(y)
y [seq(from=10,by=10,length=10)]
rpois
?rpois
z <- rpois(100,2)
z
hist(z)
z [seq(from=10,by=10,length=10)]
