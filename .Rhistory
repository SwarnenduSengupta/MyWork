geometricMean(c(0.852, 0.514, 0.512))
exp(mean(log(data[is.finite(log(data))]),na.rm=T))
geometricMean(data)
ls
data
append(data,-5)
geometricMean(data)
exp(mean(log(data[is.finite(log(data))]),na.rm=T))
data
data<-append(data,-5)
data
exp(mean(log(data[is.finite(log(data))]),na.rm=T))
geometricMean(data)
data<-data(4)
data<-data(4,)
data<-data[]
data
data<-(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
data<-[3]
data<-[,3]
data[3]
data[3]<-5.82
data[3]
geometricMean(data)
exp(mean(log(data[is.finite(log(data))]),na.rm=T))
data<-append(data,.5)
exp(mean(log(data[is.finite(log(data))]),na.rm=T))
geometricMean(data)
data[3]<-0
geometricMean(data)
exp(mean(log(data[is.finite(log(data))]),na.rm=T))
data
geometricMean(data)
library(psyh)
library(psyc)
library(psych)
geometricmean(data)
geometric.mean(data)
harmonic.mean(data)
library(psych)
ata<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
geometric.mean(data) #not useful if zero
harmonic.mean(data)
exp(mean(log(data[is.finite(log(data))]),na.rm=T))
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
geometric.mean(data) #not useful if zero
harmonic.mean(data)
exp(mean(log(data[is.finite(log(data))]),na.rm=T))
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
mean(data)
geometric.mean(data) #not useful if zero
exp(mean(log(data[is.finite(log(data))]),na.rm=T)) # handles 0
harmonic.mean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,0)
mean(data)
geometric.mean(data) #not useful if zero
exp(mean(log(data[is.finite(log(data))]),na.rm=T)) # handles 0
harmonic.mean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,0,0,0)
mean(data)
geometric.mean(data) #not useful if zero
exp(mean(log(data[is.finite(log(data))]),na.rm=T)) # handles 0
harmonic.mean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
mean(data)
geometric.mean(data) #not useful if zero in data
harmonic.mean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,NA)
mean(data,na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data,na.rm=TRUE)
mean(data,na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,NA)
mean(data)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,NA)
mean(data,na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data)
geometricMean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
mean(data,na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data)
geometricMean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,NA)
mean(data,na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data)
geometricMean(data)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,NA)
mean(data,na.rm=TRUE)
median(data,na.rm=TRUE
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data,na.rm=TRUE)
mean(data,na.rm=TRUE)
median(data,na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data,na.rm=TRUE)
mean(data,na.rm=TRUE)
median(data,na.rm=TRUE)
mode(data, na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data,na.rm=TRUE)
?mode
mean(data,na.rm=TRUE)
median(data,na.rm=TRUE)
mode(data)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data,na.rm=TRUE)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07)
mean(data,na.rm=TRUE)
median(data,na.rm=TRUE)
mode(data)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data,na.rm=TRUE)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,NA)
mean(data,na.rm=TRUE)
median(data,na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data,na.rm=TRUE)
library(psych)
data<-c(3.73,0.63, 5.81, 5.13, 0.25, 4.07,NA)
mean(data,na.rm=TRUE)
median(data,na.rm=TRUE)
geometric.mean(data,na.rm=TRUE) #not useful if zero in data
harmonic.mean(data,na.rm=TRUE)
# Clear the environment
rm(list=ls())
# Turn off scientific notations for numbers
options(scipen = 999)
# Set locale
Sys.setlocale("LC_ALL", "English")
# Set seed for reproducibility
set.seed(2345)
# Load the libraries
# Load the data
df<-read.csv(file.choose())
# count blanks remove blanks
colSums(!is.na(df))
#df <- na.omit(df)
#colSums(!is.na(df))
# clean the data names and data
names(df)<-tolower(names(df))
names(df) <- gsub("\\(","",names(df))
names(df) <- gsub("\\)","",names(df))
names(df) <- gsub("\\.","",names(df))
names(df) <- gsub("_","",names(df))
names(df) <- gsub("-","",names(df))
names(df) <- gsub(",","",names(df))
# remove a column
df$obsno <-NULL
# do the random split (25% held out for test), put the label back into the data frame
df$istest <- runif(nrow(df))<0.25
df$datalabel <- ifelse(df$istest,"test data","train data")
dftrain = df[!df$istest,]
dftest = df[df$istest,]
#remove the DF
rm(df)
# remove a column
dftrain$istest <-NULL
dftrain$datalabel <-NULL
# attach for working
attach(dftrain)
# Explore the data
str(dftrain)
summary(dftrain)
# Hierarchical Cluster Analysis
hc <- hclust(dist(dftrain))   # apply hirarchical clustering
plot(hc)                      # Print Dendrogram
# Hierarchical Cluster Analysis
# set nbr to determine how many to cut
nbr=3
di <- dist(dftrain, method="euclidean")
tree <- hclust(di, method="ward.D2")
dftrain$hcluster <- as.factor((cutree(tree, k=nbr)-2) %% 3 +1)
detach(dftrain)
attach(dftrain)
# that modulo business just makes the coming table look nicer
plot(tree, xlab="")
rect.hclust(tree, k=nbr, border="red")
#Pull out records by a value in a column
extracted <- dftrain[dftrain$hcluster==3,]
summary(extracted)
# aggregate group by
aggregate(dftrain, by=list(hcluster),FUN=mean, na.rm=TRUE)
# Compare
column <- x1
with(dftrain, table(hcluster, column))
barplot(with(dftrain, table(hcluster, column)),col=c("red","green","blue"),beside = TRUE)
median(dftest,na.rm=TRUE)
median(x336,na.rm=TRUE)
?diabetes
?diabetes
# Clear the environment
rm(list=ls())
# Turn off scientific notations for numbers
options(scipen = 999)
# Set locale
Sys.setlocale("LC_ALL", "English")
# Set seed for reproducibility
set.seed(2345)
# Load the data
df<-read.csv(file.choose())
colSums(!is.na(df))
# clean the data names and data
names(df)<-tolower(names(df))
names(df) <- gsub("\\(","",names(df))
names(df) <- gsub("\\)","",names(df))
names(df) <- gsub("\\.","",names(df))
names(df) <- gsub("_","",names(df))
names(df) <- gsub("-","",names(df))
names(df) <- gsub(",","",names(df))
# do the random split (25% held out for test), put the label back into the data frame
df$istest <- runif(nrow(df))<0.25
df$datalabel <- ifelse(df$istest,"test data","train data")
dftrain = df[!df$istest,]
dftest = df[df$istest,]
#remove the DF
rm(df)
# Hierarchical Cluster Analysis
# set nbr to determine how many to cut
nbr=2
di <- dist(dftrain, method="euclidean")
tree <- hclust(di, method="ward.D2")
dftrain$hcluster <- as.factor((cutree(tree, k=nbr)-2) %% 3 +1)
detach(dftrain)
attach(dftrain)
# that modulo business just makes the coming table look nicer
plot(tree, xlab="")
rect.hclust(tree, k=nbr, border="red")
# aggregate group by
aggregate(dftrain, by=list(hcluster),FUN=mean, na.rm=TRUE)
# remove a column
dftrain$istest <-NULL
dftrain$datalabel <-NULL
# attach for working
attach(dftrain)
# Hierarchical Cluster Analysis
# set nbr to determine how many to cut
nbr=3
di <- dist(dftrain, method="euclidean")
tree <- hclust(di, method="ward.D2")
dftrain$hcluster <- as.factor((cutree(tree, k=nbr)-2) %% 3 +1)
detach(dftrain)
attach(dftrain)
# that modulo business just makes the coming table look nicer
plot(tree, xlab="")
rect.hclust(tree, k=nbr, border="red")
aggregate(dftrain, by=list(hcluster),FUN=mean, na.rm=TRUE)
# Hierarchical Cluster Analysis
hc <- hclust(dist(dftrain))   # apply hirarchical clustering
plot(hc)                      # Print Dendrogram
# Hierarchical Cluster Analysis
# set nbr to determine how many to cut
nbr=3
di <- dist(dftrain, method="euclidean")
tree <- hclust(di, method="ward.D2")
dftrain$hcluster <- as.factor((cutree(tree, k=nbr)-2) %% 3 +1)
detach(dftrain)
attach(dftrain)
# that modulo business just makes the coming table look nicer
plot(tree, xlab="")
rect.hclust(tree, k=nbr, border="red")
aggregate(dftrain, by=list(hcluster),FUN=mean, na.rm=TRUE)
column <- diabetes
with(dftrain, table(hcluster, column))
barplot(with(dftrain, table(hcluster, column)),col=c("red","green","blue"),beside = TRUE)
# Load the data
#df<-read.csv(file.choose())
df <-read.csv("d:/data/diabetes.csv")
# count blanks remove blanks
colSums(!is.na(df))
#df <- na.omit(df)
#colSums(!is.na(df))
# clean the data names and data
names(df)<-tolower(names(df))
names(df) <- gsub("\\(","",names(df))
names(df) <- gsub("\\)","",names(df))
names(df) <- gsub("\\.","",names(df))
names(df) <- gsub("_","",names(df))
names(df) <- gsub("-","",names(df))
names(df) <- gsub(",","",names(df))
# remove a column
#df$obsno <-NULL
# do the random split (25% held out for test), put the label back into the data frame
df$istest <- runif(nrow(df))<0.25
df$datalabel <- ifelse(df$istest,"test data","train data")
dftrain = df[!df$istest,]
dftest = df[df$istest,]
#remove the DF
rm(df)
# remove a column
dftrain$istest <-NULL
dftrain$datalabel <-NULL
dftest$istest <-NULL
dftest$datalabel <-NULL
# attach for working
attach(dftrain)
summary(dftrain)
hc <- hclust(dist(dftrain))   # apply hirarchical clustering
plot(hc)
hc
summary(hc)
hc$labels
hc$order
hc$height
str(hc)
hc$hclust
hc$method
hc$labels
column <- diabetes
with(dftrain, table(hcluster, column))
# Hierarchical Cluster Analysis
# set nbr to determine how many to cut
nbr=3
di <- dist(dftrain, method="euclidean")
tree <- hclust(di, method="ward.D2")
dftrain$hcluster <- as.factor((cutree(tree, k=nbr)-2) %% 3 +1)
detach(dftrain)
attach(dftrain)
# that modulo business just makes the coming table look nicer
plot(tree, xlab="")
rect.hclust(tree, k=nbr, border="red")
# Compare
column <- diabetes
with(dftrain, table(hcluster, column))
column <- age
with(dftrain, table(hcluster, column))
column <- timespregnant
with(dftrain, table(hcluster, column))
library(psych)
mean(dftrain$age,na.rm=TRUE)
median(dftrain$age,na.rm=TRUE)
harmonic.mean(dftrain$age,na.rm=TRUE)
# Measures of Central Tendency
mean(dftrain$age,na.rm=TRUE)
median(ftrain$age,na.rm=TRUE)
# REQUIRES psych package
geometric.mean(ftrain$age,na.rm=TRUE) #not useful if zero in data
harmonic.mean(ftrain$age,na.rm=TRUE)
# Measures of Central Tendency
mean(dftrain$age,na.rm=TRUE)
median(dftrain$age,na.rm=TRUE)
# REQUIRES psych package
geometric.mean(dftrain$age,na.rm=TRUE) #not useful if zero in data
harmonic.mean(dftrain$age,na.rm=TRUE)
Clear the environment
rm(list=ls())
# Turn off scientific notations for numbers
options(scipen = 999)
# Set locale
Sys.setlocale("LC_ALL", "English")
# Set seed for reproducibility
set.seed(2345)
# Load the libraries
library(psych)
# Load the data
#df<-read.csv(file.choose())
df <-read.csv("d:/data/diabetes.csv")
# count blanks remove blanks
colSums(!is.na(df))
#df <- na.omit(df)
#colSums(!is.na(df))
# clean the data names and data
names(df)<-tolower(names(df))
names(df) <- gsub("\\(","",names(df))
names(df) <- gsub("\\)","",names(df))
names(df) <- gsub("\\.","",names(df))
names(df) <- gsub("_","",names(df))
names(df) <- gsub("-","",names(df))
names(df) <- gsub(",","",names(df))
# remove a column
#df$obsno <-NULL
# do the random split (25% held out for test), put the label back into the data frame
df$istest <- runif(nrow(df))<0.25
df$datalabel <- ifelse(df$istest,"test data","train data")
dftrain = df[!df$istest,]
dftest = df[df$istest,]
#remove the DF
rm(df)
# remove a column
dftrain$istest <-NULL
dftrain$datalabel <-NULL
dftest$istest <-NULL
dftest$datalabel <-NULL
attach(dftrain)
library(rpart)
fit <- rpart(timespregnant ~ ., method="class", data=dftrain)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
library(rpart)
fit <- rpart(diabetes ~ ., method="class", data=dftrain)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# plot tree
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postscript plot of tree
post(fit, file = "c:/tree.ps",
title = "Classification Tree for Kyphosis")
# prune the tree
pfit<- prune(fit, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
# plot the pruned tree
plot(pfit, uniform=TRUE,
main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
post(pfit, file = "c:/ptree.ps",
title = "Pruned Classification Tree for Kyphosis")
fit <- rpart(diabetes ~ .,method="anova", data=dftrain)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# create additional plots
par(mfrow=c(1,2)) # two plots on one page
rsq.rpart(fit) # visualize cross-validation results
# plot tree
plot(fit, uniform=TRUE,
main="Regression Tree for Mileage ")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postcript plot of tree
post(fit, file = "c:/tree2.ps",
title = "Regression Tree for Mileage ")
# Regression Tree
library(rpart)
# grow tree
fit <- rpart(diabetes ~ .,method="anova", data=dftrain)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# create additional plots
rsq.rpart(fit) # visualize cross-validation results
# plot tree
plot(fit, uniform=TRUE, main="Regression Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postcript plot of tree
post(fit, file = "c:/tree2.ps", title = "Regression Tree ")
# Regression Tree
library(rpart)
# grow tree
fit <- rpart(diabetes ~ .,method="anova", data=dftrain)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# create additional plots
rsq.rpart(fit) # visualize cross-validation results
# plot tree
plot(fit, uniform=TRUE, main="Regression Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postcript plot of tree
post(fit, file = "c:/tree2.ps", title = "Regression Tree ")
# prune the tree
pfit<- prune(fit, cp=0.01160389) # from cptable
# plot the pruned tree
plot(pfit, uniform=TRUE, main="Pruned Regression Tree")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
post(pfit, file = "c:/ptree2.ps",title = "Pruned Regression Tree")
library(randomForest)
Kyphosis ~ Age + Number + Start
library(randomForest)
fit <- randomForest(diabetes ~ .,   data=dftrain)
print(fit) # view results
importance(fit) # importance of each predictor
print(fit) # view results
importance(fit) # importance of each predictor
fit <- randomForest(as.facotr(diabetes) ~ .,   data=dftrain)
fit <- randomForest(as.factor(diabetes) ~ .,   data=dftrain)
print(fit) # view results
importance(fit) # importance of each predictor
plot(fit)
varImpPlot(fit)
summary(fit) # detailed summary of splits
Prediction <- predict(fit, dftest)
Prediction
table(dftest)
summary(Prediciton)
summary(Predicition)
summary(Prediction)
table(dftest$diabetes)
?accuracy
confusion
?confusion
?Confusion
table(dftest$diabetes,Prediction)
table(Prediction,dftest$diabetes)
library(party)
install.packages('party')
library(party)
fit <- cforest(as.factor(diabetes) ~ .,   data = dftrain, controls=cforest_unbiased(ntree=2000, mtry=3))
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
Prediction <- predict(fit, dftest, OOB=TRUE, type = "response")
table(Prediction)
table(Prediction,dftest$diabetes)
table(dftest$diabetes,Prediction)
sum(dftest$diabetes)
183-69
library(rattle)
rattle()
library(rattle)
rattle()
