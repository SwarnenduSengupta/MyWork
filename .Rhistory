'qcc', # statistical quality control and QC charts
'mice', # multiple imputation using Fully Conditional Specification (FCS)
## Reporting
'xtable', # Export tables to LaTeX or HTML
'knitr', #  A general-purpose package for dynamic report generation in R
'stargazer', # LaTeX/HTML code and ASCII text for well-formatted regression and summary statistics tables
## Miscellaneous
# 'sendmailR', # send email using R
'devtools', # Tools to make developing R code easier
'gridExtra', # misc. high-level Grid functions
'twitteR', # R based Twitter client
'doMC', # for multi-core processing
'packrat' # Reproducible package management for R, more info @ http://rstudio.github.io/packrat/
)
install.packages(x)
rm(x)
install.packages(rattle, dependencies = c("Depends", "Suggests"))
install.packages("rattle")
install.packages("twitteR")
install.packages("twitteR")
###
### Read tweets from Twitter using ATOM (XML) format
###
# installation is required only required once and is rememberd across sessions
install.packages('XML')
# loading the package is required once each session
require(XML)
# initialize a storage variable for Twitter tweets
mydata.vectors <- character(0)
# paginate to get more tweets
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
# how many tweets did we get?
length(mydata.vectors)
###
### Read tweets from Twitter using ATOM (XML) format
###
# installation is required only required once and is rememberd across sessions
install.packages('XML')
# loading the package is required once each session
require(XML)
# initialize a storage variable for Twitter tweets
mydata.vectors <- character(0)
# paginate to get more tweets
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
# how many tweets did we get?
length(mydata.vectors)
install.packages("XML")
# loading the package is required once each session
require(XML)
# initialize a storage variable for Twitter tweets
mydata.vectors <- character(0)
# paginate to get more tweets
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
# how many tweets did we get?
length(mydata.vectors)
mydata.vectors <- character(0)
twitter_q <- URLencode('#prolife OR #prochoice')
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
library(nycflights13)
install.packages("nycflights13")
library(nycflights13)
attach(flights)
filter(flights, month == 1, day == 1)
filter(flights, month == 1)
library(dplyr)
filter(flights, month == 1, day == 1)
summarise(flights,
delay = mean(dep_delay, na.rm = TRUE))
sample_n(flights, 10)
library(twitteR)
rdmTweets <- userTimeline("rdatamining", n=100)
setup_twitter_oauth("API key", "API secret")
library(twitteR)
setup_twitter_oauth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
?twitteR
load("C:/Users/bryan_000/Downloads/rdmTweets-201306.RData")
library(httr)
setup_twitter_oauth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
setup_twitteR_oauth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
twitteR_oauth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
getTwitterOAuth(consumer_key, consumer_secret)
registerTwitterOAuth(oauth)
getTwitterOAuth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
getTwitterOAuth("79261207-rMLPaWDKKkPg7iwjrNHq0yVX9vEKtYkSVGh7CAluj", "JgFqyPrKcqodyRZ1ApxuzOeTXfrlj7fQpcoX9FXF21Ndt")
getTwitterOAuth("79261207-rMLPaWDKKkPg7iwjrNHq0yVX9vEKtYkSVGh7CAluj", "JgFqyPrKcqodyRZ1ApxuzOeTXfrlj7fQpcoX9FXF21Ndt")
getTwitterOAuth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
getTwitterOAuth("jYEGXw87lkgjitQqUIBVcnUzE", "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm")
cred <- OAuthFactory$new(consumerKey='cred <- OAuthFactory$new(consumerKey='secret',
consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='http://api.twitter.com/oauth/access_token',
authURL='http://api.twitter.com/oauth/authorize')',
consumerSecret='secret',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='http://api.twitter.com/oauth/access_token',
authURL='http://api.twitter.com/oauth/authorize')
cred <- OAuthFactory$new(consumerKey='cred <- OAuthFactory$new(consumerKey='secret',
consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='http://api.twitter.com/oauth/access_token',
authURL='http://api.twitter.com/oauth/authorize')
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='http://api.twitter.com/oauth/access_token',authURL='http://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
8461425
cred$handshake(cainfo="cacert.pem")
#install the necessary packages
install.packages("ROAuth")
install.packages("twitteR")
install.packages("wordcloud")
install.packages("tm")
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
#necessary step for Windows
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
#to get your consumerKey and consumerSecret see the twitteR documentation for instructions
cred <- OAuthFactory$new(consumerKey='secret',
consumerSecret='secret',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='http://api.twitter.com/oauth/access_token',
authURL='http://api.twitter.com/oauth/authorize')
#necessary step for Windows
cred$handshake(cainfo="cacert.pem")
#save for later use for Windows
save(cred, file="twitter authentication.Rdata")
registerTwitterOAuth(cred)
#the cainfo parameter is necessary on Windows
r_stats<- searchTwitter("#Rstats", n=1500, cainfo="cacert.pem")
#save text
r_stats_text <- sapply(r_stats, function(x) x$getText())
#create corpus
r_stats_text_corpus <- Corpus(VectorSource(r_stats_text))
#clean up
r_stats_text_corpus <- tm_map(r_stats_text_corpus, tolower)
r_stats_text_corpus <- tm_map(r_stats_text_corpus, removePunctuation)
r_stats_text_corpus <- tm_map(r_stats_text_corpus, function(x)removeWords(x,stopwords()))
wordcloud(r_stats_text_corpus)
install.packages("twitteR")
save(cred, file="twitter authentication.Rdata")
install.packages("twitteR")
install.packages("twitteR")
install.packages("twitteR")
save(cred, file="twitter authentication.Rdata")
registerTwitterOAuth(cred)
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='http://api.twitter.com/oauth/access_token',authURL='http://api.twitter.com/oauth/authorize')
library(twitteR)
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='http://api.twitter.com/oauth/access_token',authURL='http://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
twitCred = c(consumerKey = ' jYEGXw87lkgjitQqUIBVcnUzE', consumerSecret = ' oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',oauthKey = ' 79261207-rMLPaWDKKkPg7iwjrNHq0yVX9vEKtYkSVGh7CAluj',oauthSecret = ' JgFqyPrKcqodyRZ1ApxuzOeTXfrlj7fQpcoX9FXF21Ndt')
curl = getCurlHandle()
options(RCurlOptions = list(capath = system.file('CurlSSL', 'cacert.pem',
package = 'RCurl'), ssl.verifypeer = FALSE))
curlSetOpt(.opts = list(proxy = 'proxyserver:port'), curl = curl)
library("twitteR")
library("ROAuth")
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='http://api.twitter.com/oauth/access_token',authURL='http://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
save(cred, file="twitter authentication.Rdata")
cred <- OAuthFactory$new(consumerKey='jYEGXw87lkgjitQqUIBVcnUzE',consumerSecret='oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm',requestURL='https://api.twitter.com/oauth/request_token',accessURL='https://api.twitter.com/oauth/access_token',authURL='https://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
search.string <- "#nba"
no.of.tweets <- 100
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
cred$handshake(cainfo="cacert.pem")
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "jYEGXw87lkgjitQqUIBVcnUzE"
consumerSecret <- "oOsXCQQvdzujOamHALWLWB4Hv22Whjilou7gcspTAk1jydTBcm"
twitCred <- OAuthFactory$new(consumerKey=consumerKey,consumerSecret=consumerSecret,requestURL=reqURL,accessURL=accessURL,authURL=authURL)
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
registerTwitterOAuth(twitCred)
tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en")
tweets
source('~/GitHub/MyWork/TwitterLogin.R')
registerTwitterOAuth(twitCred)
search.string <- "#highered"; no.of.tweets <- 100; tweets <- searchTwitter(search.string, n=no.of.tweets, cainfo="cacert.pem", lang="en");tweets
source('~/GitHub/MyWork/TwitterGetTweets.R')
tweets
un.tweets = searchTwitter('@United',n=1000, cainfo="cacert.pem")
length(un.tweets)
un.tweets
rm(tweets)
library(plyr)
un.text = laply(un.tweets, function(t)t$getText())
un.text
tr <-getTrends(’weekly’, as.character(Sys.Date()-1))
tr <-getTrends(’weekly’, as.character(Sys.Date()-1))
library(tm)
data.corpus <- Corpus(VectorSource(un.text))
data.corpus <- tm_map(data.corpus, tolower)
data.corpus <- t_map(data.corpus, removePunctuation)
# remove stop words, otherwise some of these words would appear as most used
> some_stopwords <- c(stopwords('english'))
> data.corpus <- tm_map(data.corpus, removeWords, some_stopwords)
some_stopwords <- c(stopwords('english'))
data.corpus <- tm_map(data.corpus, removeWords, some_stopwords)
data.corpus
data.corpus <- TermDocumentMatrix(data.corpus)
data.corpus
data.tm
inspect(data.corpus[1:1000])
un.text
inspect(data.corpus[1:1000])
data.corpus <- t_map(data.corpus, removePunctuation)
library(tm)
install.packages("tm")
library(tm)
data.corpus <- t_map(data.corpus, removePunctuation)
library(TwitteR)
library(twitteR)
data.corpus <- t_map(data.corpus, removePunctuation)
library(plyr)
data.corpus <- t_map(data.corpus, removePunctuation)
data.corpus <- tm_map(data.corpus, removePunctuation)
data.corpus <- tm_map(data.corpus, tolower)
inspect(data.corpus[1:1000])
data.corpus <- TermDocumentMatrix(data.corpus)
data.corpus <- tm_map(data.corpus, removeWords, some_stopwords)
data.corpus
data.tm
data.corpus [[116]]
findFreqTerms(data.dtm, lowfreq=30)
findFreqTerms(data.corpus, lowfreq=30)
tr <- getTrends(’weekly’, as.character(Sys.Date()-1))
tr <- getTrends(’weekly’, as.character(Sys.Date()-1))
data.dtm <- TermDocumentMatrix(data.corpus)
delta.tweets <- searchTwitter('@delta', n=1000)
source('~/GitHub/MyWork/TwitterGetTweets.R')
load("~/GitHub/R/twitter authentication.Rdata")
source('~/GitHub/MyWork/TwitterGetTweets.R')
source('~/GitHub/MyWork/TwitterGetTweets.R')
source('~/GitHub/MyWork/TwitterLogin.R')
source('~/GitHub/MyWork/TwitterGetTweets.R')
tweets
source('~/GitHub/MyWork/TwitterGetTweets.R')
USArrests
library(psych)
fit <- principal(USArrests, nfactors=, rotate="varimax")
fit
install.packages("nFactors")
?pa
?fa
Harman74
require(stats)
Harman74
Harman74.cor
install.packages("FactoMineR")
data(us.bis.yield)
library("Rsafd")
install.package("Rsafd")
install.packages("Rsafd")
install.packages("Rsafd")
data(us.bis.yield)
gender = rep(c("female","male"),c(1835,2691))
admitted = rep(c("yes","no","yes","no"),c(557,1278,1198,1493))
dept = rep(c("A","B","C","D","E","F","A","B","C","D","E","F"),
c(89,17,202,131,94,24,19,8,391,244,299,317))
dept2 = rep(c("A","B","C","D","E","F","A","B","C","D","E","F"),
c(512,353,120,138,53,22,313,207,205,279,138,351))
department = c(dept,dept2)
ucb = data.frame(gender,admitted,department)
rm(gender,admitted,dept,dept2,department)
ls()
rm(accessURL)
rm(cred)
rm(un.text)
rm(un.tweets)
ls()
rm(authURL)
rm(reqURL)
rm(consumerKey)
rm(curl)
rm(no.of.tweets)
rm(reqURL)
ls()
rm(data.corpus)
rm(search.string)
rm(some_stopwords)
rm(tweets)
rm(twitCred)
rm(consumerSecret)
ucb
save(ucb)
?save
save(ucb, file = "ucb.RData")
str(ucb)
summary(ucb)
table(ucb$gender)
table(ucb$admitted)
table(ucb$department) -> dept.table
prop.table(dept.table)
prop.table(dept.table) * 100
with(ucb, table(gender, admitted))
xtabs(~ gender + admitted, data=ucb)
xtabs(~ gender + admitted, data=ucb) -> gen.adm.table
data(faithful)
stem(waiting)
stem(faithful$waiting)
as.data.frame(state.x77) -> states
states$region <- state.region
by(data=states[4], IND=states[9], FUN=mean)
gender = rep(c("female","male"),c(1835,2691))
rm(list=ls())
install.packages("RFacebook")
install.packages("Rfacebook")
install.packages("Rook")
Require(Rfacebook)
require(Rfacebook)
require(Rook)
fb_oauth <- fbOAuth(app_id="176988012511120", app_secret="991efd8aba85c69235c4431b57dc12b8")
fb_oauth <- fbOAuth(app_id="176988012511120", app_secret="CAACEdEose0cBAAKXSOJVin1FLfuMBrzDVxXX7zPNHm28NAA2izuTKlN41zg9Xd7202qT6wKG7gwcn5ViI9qhm20WQQZCdwKNXPLJn7lNRaxFG46RAWo9NpAtwTgk28Nh3fITiN7cKv2g7vHmtkbDmowBk11oS37ZBg9mX2mzuMzBYDt0vz14jmnhaiJQKOvEZATGOlnftKquXZBqAxI8vRtbcpYmU24ZD")
fb_oauth <- fbOAuth(app_id="145634995501895", app_secret="CCAACEdEose0cBAAhAlvU0R49tOx5CZAj9zdXOmo5F54Ks869XdzuNKov9GeiQrCwnPO0kuSSte6WDZAu9jGNxayoo4wn6Ery9hDv6pMSDam9OM0CRpPw2obrNHtZAz01BMwrUQntEgwZCsfebacN4hcXvniYqkJItD0OF9ZC1ZB4Mcg39GfudgMH46ZBiOFVWZB6RbDUHkZBT0EoVnle00Nb3IK3OlZBZBj47w8ZD")
me <- getUsers("me", token=fb_oauth)
fb_oauth <- fbOAuth(app_id="145634995501895", app_secret="CCAACEdEose0cBAAhAlvU0R49tOx5CZAj9zdXOmo5F54Ks869XdzuNKov9GeiQrCwnPO0kuSSte6WDZAu9jGNxayoo4wn6Ery9hDv6pMSDam9OM0CRpPw2obrNHtZAz01BMwrUQntEgwZCsfebacN4hcXvniYqkJItD0OF9ZC1ZB4Mcg39GfudgMH46ZBiOFVWZB6RbDUHkZBT0EoVnle00Nb3IK3OlZBZBj47w8ZD")
token = "CAACEdEose0cBAKTTc22dkBPzz2Bj6ZAOprah478cKIRSYMKLXllIwtTBDUYk9gZABmaxHUDPLBRsLirVCGIaj7FnW0QZA2PbZCiZBZBFwnpztr2Man37wyqgngwYdKyK2jW3dyCnlMFJIe5ZAf91UVPzxKOmnlGdoUZCRLlM7QGAVdOPlJZCmGiUSTdghyPqQYN5IJdKrRs0WEGexjOIjTn0qDNZC9vPHkUZAkZD"
me <- getUsers("me", token=token)
me
my_friends <- getFriends(token, simplify=TRUE)
my_friends_info <- getUsers(my_friends$id, token=token, private_info=TRUE)
table(my_friends_info$location)
my_friends
my_friends <- getFriends(token)
str(me)
str(my_friends)
getFriends(token, simplify = FALSE)
rm(list(ls()))
rm(list(ls())
)
rm(list=ls())
list=ls()
rm(list=ls())
data(mtcars)
str(mtcars)
bestmodel <- step(inclusivemodel, direction = "both")
summary(bestmodel)
inclusivemodel <- lm(mpg ~ ., data = mtcars)
bestmodel <- step(inclusivemodel, direction = "both")
summary(bestmodel)
plot(bestmodel$residualks)
plot(bestmodel$residuals)
abline(o)
abline(0)
abline(0,0)
hist(bestmodel$residuals)
qqnorm(bestmodel$residuals)
abline(bestmodel$residuals)
qqplot(bestmodel$residuals)
qqline(bestmodel$residuals,)
plot(bestmodel$residuals~bestmodel$fitted)
abline(0,0)
plot(abs(bestmodel$residuals)~bestmodel$fitted)
plot(bestmodel$residuals~mtcars$am)
plot(bestmodel$residuals~mtcars$wt)
plot(bestmodel$residuals~mtcars$qsec)
abline(0,0)
plot(bestmodel$residuals~mtcars$am)
plot(bestmodel$residuals~mtcars$am)
rm(list=ls())
am.glm = glm(formula=am ~ hp + wt,
+              data=mtcars,
+              family=binomial)
am.glm = glm(formula=am ~ hp + wt,
data=mtcars,
family=binomial)
summary(am.glm)
newdata = data.frame(hp=120, wt=2.8)
predict(am.glm, newdata, type="response")
d <- dist(as.matrix(mtcars))
hc <- hclust(d)
plot(hc)
summary(hc)
smoke <- as.numeric(factor(survey$Smoke,
levels=c("Never","Occas","Regul","Heavy")))  exer <- as.numeric(factor(survey$Exer,
levels=c("None","Some","Freq")))
smoke <- as.numeric(factor(survey$Smoke,
levels=c("Never","Occas","Regul","Heavy")))
source('C:/Users/bryan_000/Downloads/Exploratory/script.R')
library(ISLR)
install.packages("ISLR")
library(ISLR)
attach(Smarket)
str(Smarket)
cor(Smarket)
cor(Smarket[,-9])
pairs(Smarket[,-9])
options(scipen=999) # suppressing scientific notation
library(party)
if (!require("party")){
install.packages("party")
}
library(party)
library(fpc)
if (!require("fpc")){
install.packages("fpc")
}
setwd("~/GitHub/MyWork")
mydata <- read.csv("KidCreative.csv")
mydata
str(mydata)
attach(mydata)
glm(Buy~.)
glm(Buy~., data=mydata, family=binomial)
mylogit <-glm(Buy~., data=mydata, family=binomial)
summary(mylogit)
options(scipen=999) # suppressing scientific notation
summary(mylogit)
exp(cbind(OR = coef(mylogit), confint(mylogit)))
source('~/GitHub/MyWork/KidCreativeAnalysis.R')
exp(cbind(OR = coef(mylogit), confint(mylogit)))
source('~/GitHub/MyWork/KidCreativeAnalysis.R')
mylogit <-lm(Buy~., data=mydata)
mylogit
summary(mylogit)
str(mylogit)
str(mydata)
mydata <-mydata[,-1]
str(mydata)
mylogit <-lm(Buy~., data=mydata)
summary(mylogit)
mylogit <-lm(Incomew~., data=mydata)
mylogit <-lm(Income~., data=mydata)
summary(mylogit)
source('~/GitHub/MyWork/KidCreativeAnalysis.R')
summary(mylogit)
exp(cbind(OR = coef(mylogit), confint(mylogit)))
exp(cbind(OR = coef(mylogit), confint(mylogit), Prob=OR-1*100))
exp(cbind(OR = coef(mylogit), confint(mylogit), Prob =coef(mylogit)-1*100))
exp(cbind(OR = coef(mylogit), confint(mylogit), Prob =(coef(mylogit)-1)*100))
exp(cbind(OR = coef(mylogit), confint(mylogit))),Prob =exp(coef(mylogit)-1)*100)
exp(cbind(OR = coef(mylogit), confint(mylogit))),Prob =exp(cbind(coef(mylogit)-1)*100))
exp(coef(mylogit)-1)*100)
exp(coef(mylogit)-1)*100
exp((coef(mylogit))-1)*100
(exp(coef(mylogit))-1)*100
source('~/GitHub/MyWork/KidCreativeAnalysis.R')
exp(cbind(OR = coef(mylogit), confint(mylogit)))
(exp(coef(mylogit))-1)*100
cbind(exp(coef(mylogit))-1)*100)
cbind((exp(coef(mylogit))-1)*100)
cbind((exp(coef(mylogit))-1)*100)
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
mydata$rank <- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
fittedvalues <- fitted(mylogit)
plot(mylogit$fitted)
points (mylogit$admit, mylogit$gre + mylogit$gpa + mylogit$rank, col="red")
points (mylogit$admit, mylogit$gre, col="red")
points (mylogit$admitcol="red")
points (mylogit$admit,col="red")
plot(admit, fitted.values(nylogit))
plot(mylogit$admitcol="red")
Error: unexpected 'admit, fitted.values(nylogit))
plot(admit, fitted.values(mylogit))
)
)))
plot(admit, fitted.values(mylogit))
plot(mylogit$admit, fitted.values(mylogit))
ploy(mylogit$admit, mylogit$admit$rank)
plot(mylogit$admit, mylogit$admit$rank)
plot(mylogit$admit, mylogit$admit$gre)
plot(mydata$admit, mydata$admit$gre)
plot(mydata$admit, mydata$gre)
ploy(mydata$admit, mydata$rank)
plot(mydata$admit, mydata$rank)
plot(mydata$rank,mydata$admit)
plot(mydata$gre,mydata$admit)
plot(mydata$gpa,mydata$admit)
