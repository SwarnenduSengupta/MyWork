fit
predictTrain = predict(fit, type="response")
summary(predictTrain)
tapply(predictTrain, dftrain$PoorCare, mean)
predictTrain = predict(fit, type="response")
summary(predictTrain)
tapply(predictTrain, dftrain$poorcare, mean)
source('~/.active-rstudio-document', echo=TRUE)
xtabs(~get(depvar) + get(indepvars), data = dftrain)
dftrain$poorcare <-as.factor(dftrain$poorcare)
View(dftest)
View(dftrain)
# Dep and Independent Vars define columns we will be working with
depvar <- 'poorcare'
indepvars <-c('pain','totalvisits')
indepvars <-c('startedoncombination', 'providercount')
#  to make sure there are not 0 cells
xtabs(~get(depvar) + get(indepvars), data = dftrain)
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
#Fit the model
fit<-glm(f1,data=dftrain,family=binomial)
fit
xtabs(~get(depvar) + get(indepvars), data = dftrain)
indepvars <-c('startedoncombination')
xtabs(~get(depvar) + get(indepvars), data = dftrain)
fit<-glm(f1,data=dftrain,family=binomial)
fit
indepvars <-c('startedoncombination')
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
xtabs(~get(depvar) + get(indepvars), data = dftrain)
fit<-glm(f1,data=dftrain,family=binomial)
fit
indepvars <-c('.')
xtabs(~get(depvar) + get(indepvars), data = dftrain)
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
fit<-glm(f1,data=dftrain,family=binomial)
fit
predictTrain = predict(fit, type="response")
summary(predictTrain)
tapply(predictTrain, dftrain$poorcare, mean)
predictTrain = predict(fit, type="response",newdata=dftest)
predictTrain = predict(fit, newdata=dftest)
dftest$poorcare <-as.factor(dftest$poorcare)
predictTrain = predict(fit, newdata=dftest)
summary(predictTrain)
predictTrain = predict(fit, newdata=dftest)
predictTrain = predict(fit, newdata=dftest, type="response)"
)
predictTrain = predict(fit, newdata=dftest, type="response")
summary(predictTrain)
tapply(predictTrain, dftrain$poorcare, mean)
tapply(predictTrain, dftest$poorcare, mean)
?predict
summary(predictTrain)
table(dftrain$poorcare,predictTrain>thres)
thres=.5
table(dftrain$poorcare,predictTrain>thres)
depvar <- 'poorcare'
indepvars <-c('pain','totalvisits')
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
#Fit the model
fit<-glm(f1,data=dftrain,family=binomial)
fit
predictTrain = predict(fit, type="response")
summary(predictTrain)
tapply(predictTrain, dftrain$poorcare, mean)
thres=.5
table(dftrain$poorcare,predictTrain>thres)
depvar <- 'poorcare'
indepvars <-c('narcotics','officevisits')
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
fit<-glm(f1,data=dftrain,family=binomial)
fit
predictTrain = predict(fit, type="response")
summary(predictTrain)
tapply(predictTrain, dftrain$poorcare, mean)
#Confusion Matrix (row,column>threshold) (actual,prediciton>threshold)
thres=.5
table(dftrain$poorcare,predictTrain>thres)
10/25
70/74
thres=.7
table(dftrain$poorcare,predictTrain>thres)
#Sensitivity ()
10/25
#Specificity
70/74
#Sensitivity ()
8/25
#Specificity
73/74
thres=.2
table(dftrain$poorcare,predictTrain>thres)
#Sensitivity ()
16/25
#Specificity
54/74
20/5
20/25
15/25
15/25
library(ROCR)
ROCRpred <-prediction(predictTrain, dftrain$poorcare)
ROCRpred
ROCRperf <-performance(ROCRpred, "tpr","fpr")
ROCRperf
pplot(ROCRperf)
plot(ROCRperf)
plot(ROCRperf, colorize=TRUE)
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,.1))
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,.1), text.adj=c(-.2, 1.7))
cm <-table(dftrain$poorcare,predictTrain>thres)
str(cm)
cm[1,1]
cm
tpr <- cm[2,2] / (cm[2,1]+cm[2/2])
tpr
16/25
54/74
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
tpr
16/25
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
54/74
tnr
cm <-table(dftrain$poorcare,predictTrain>thres)
#Sensitivity (True positive rate)
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
#Specificity (true negitive rate)
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
#False Positive Rate
fpr <- 1-tnr
# False Negitive rate
fnr <- 1-tpr
fnr
fpe
fper
fpr
tpr
tnr
rbind(TPR=tpr,FPR,fpr)
rbind(TPR=tpr,FPR=fpr)
rbind(TNR=tnr,FNR=fnr)
cbinf(rbind(TPR=tpr,FPR=fpr),rbind(TNR=tnr,FNR=fnr))
cbind(rbind(TPR=tpr,FPR=fpr),rbind(TNR=tnr,FNR=fnr))
cbind(T=rbind(TPR=tpr,FPR=fpr),F=rbind(TNR=tnr,FNR=fnr))
rbind(TPR=tpr,FPR=fpr)
rbind(TNR=tnr,FNR=fnr))
rbind(TPR=tpr,FPR=fpr)
rbind(TNR=tnr,FNR=fnr)
predictTest = predict(fit, type="response", newdata="dftest")
predictTest = predict(fit, type="response", newdata=dftest)
summary(oredictTest)
cbind(T=rbind(TPR=tpr,FPR=fpr),F=rbind(TNR=tnr,FNR=fnr))
summary(predictTest)
# Analyze predictions
summary(predictTest)
tapply(predictTest, dftest$poorcare, mean)
#Confusion Matrix (row,column>threshold) (actual,prediciton>threshold)
thres=.2
cm <-table(dftest$poorcare,predictTest>thres)
cm
thres=.5
cm <-table(dftest$poorcare,predictTest>thres)
cm
thres=.3
cm <-table(dftest$poorcare,predictTest>thres)
cm
#Sensitivity (True positive rate)
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
#Specificity (true negitive rate)
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
#False Positive Rate
fpr <- 1-tnr
# False Negitive rate
fnr <- 1-tpr
rbind(TPR=tpr,FPR=fpr)
rbind(TNR=tnr,FNR=fnr)
6/8
5/25
rbind(Sensitivity(TPR)=tpr,Specificity(FPR)=fpr)
rbind(TNR=tnr,FNR=fnr)
rbind(Sensitivity=tpr,Specificity=fpr)
rbind(TNR=tnr,FNR=fnr)
rbind(Sensitivity.TPR=tpr,Specificity.FPR=fpr)
rbind(TNR=tnr,FNR=fnr)
rbind(FalseNegativeRate=fnr,TrueNegtiveRate=tnr)
rbind(TrueNegativeRate=tnr,FalseNegativeRate=fnr,TrueNegativeRate=tnr)
rbind(TruePositiveRate=tpr,FalseNegativeRate=fnr,TrueNegativeRate=tnr)
rbind(TruePositiveRate=tpr,FalseNegativeRate=fnr,TrueNegativeRate=tnr,FalsePositiveRate=fpr)
thres=.2
cm <-table(dftrain$poorcare,predictTrain>thres)
#Sensitivity (True positive rate)
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
#Specificity (true negitive rate)
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
#False Positive Rate
fpr <- 1-tnr
# False Negitive rate
fnr <- 1-tpr
#
rbind(TruePositiveRate=tpr,FalseNegativeRate=fnr,TrueNegativeRate=tnr,FalsePositiveRate=fpr)
#Confusion Matrix (row,column>threshold) (actual,prediciton>threshold)
thres=.3
cm <-table(dftrain$poorcare,predictTrain>thres)
#Sensitivity (True positive rate)
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
#Specificity (true negitive rate)
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
#False Positive Rate
fpr <- 1-tnr
# False Negitive rate
fnr <- 1-tpr
#
rbind(TruePositiveRate=tpr,FalseNegativeRate=fnr,TrueNegativeRate=tnr,FalsePositiveRate=fpr)
#Confusion Matrix (row,column>threshold) (actual,prediciton>threshold)
thres=.3
cm <-table(dftest$poorcare,predictTest>thres)
#Sensitivity (True positive rate)
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
#Specificity (true negitive rate)
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
#False Positive Rate
fpr <- 1-tnr
# False Negitive rate
fnr <- 1-tpr
rbind(TruePositiveRate=tpr,FalseNegativeRate=fnr,TrueNegativeRate=tnr,FalsePositiveRate=fpr)
source('~/GitHub/MyWork/EdExUnit3InCLass.R', echo=TRUE)
# Load functions
source('functions.R')
# Load the data
df<-read.csv("D:/Data/quality.csv")
df<-cleanit(df)
# count blanks remove blanks
colSums(!is.na(df))
#df <- na.omit(df)
#colSums(!is.na(df))
# Cretae test and training sets
set.seed
library(caTools)
set.seed(88)
split = sample.split(df$poorcare, SplitRatio = 0.75)
#split
dftrain = subset(df, split == TRUE)
dftest = subset(df, split == FALSE)
# If you wanted to instead split a data frame data, where the dependent variable is a continuous outcome
# you could instead use the sample() function. Here is how to select 70% of observations for the training set
# (called "train") and 30% of observations for the testing set (called "test"):
# spl = sample(1:nrow(data), size=0.7 * nrow(data))
# train = data[spl,]
# test = data[-spl,]
# Dep and Independent Vars define columns we will be working with
depvar <- 'poorcare'
indepvars <-c('narcotics','officevisits')
# two-way contingency table of categorical outcome and predictors we want
#  to make sure there are not 0 cells
xtabs(~get(depvar) + get(indepvars), data = dftrain)
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
#Fit the model
fit<-glm(f1,data=dftrain,family=binomial)
fit
# Make predictions (probabilities) on training set
predictTrain = predict(fit, type="response")
# Analyze predictions
summary(predictTrain)
tapply(predictTrain, dftrain$poorcare, mean)
#Confusion Matrix (row,column>threshold) (actual,prediciton>threshold)
thres=.3
cm <-table(dftrain$poorcare,predictTrain>thres)
#Sensitivity (True positive rate)
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
#Specificity (true negitive rate)
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
#False Positive Rate
fpr <- 1-tnr
# False Negitive rate
fnr <- 1-tpr
#
rbind(TruePositiveRate=tpr,FalseNegativeRate=fnr,TrueNegativeRate=tnr,FalsePositiveRate=fpr)
# Generate ROC
library(ROCR)
# first get predictions using ROCR
ROCRpred <-prediction(predictTrain, dftrain$poorcare)
# Use the predictions generated to create performace for tpr and fpr
ROCRperf <-performance(ROCRpred, "tpr","fpr")
# Plot
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,.1), text.adj=c(-.2, 1.7))
# Make predictions (probabilities) on training set
predictTest = predict(fit, type="response", newdata=dftest)
# Analyze predictions
summary(predictTest)
tapply(predictTest, dftest$poorcare, mean)
#Confusion Matrix (row,column>threshold) (actual,prediciton>threshold)
thres=.3
cm <-table(dftest$poorcare,predictTest>thres)
#Sensitivity (True positive rate)
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
#Specificity (true negitive rate)
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
#False Positive Rate
fpr <- 1-tnr
# False Negitive rate
fnr <- 1-tpr
#
rbind(TruePositiveRate=tpr,FalseNegativeRate=fnr,TrueNegativeRate=tnr,FalsePositiveRate=fpr)
# first get predictions using ROCR
ROCRpredTest <-prediction(predictTest, dftest$poorcare)
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
auc
#ROC
# first get predictions using ROCR
ROCRpredTest <-prediction(predictTest, dftest$poorcare)
# Use the predictions generated to create performace for tpr and fpr
ROCRperfTest <-performance(ROCRpredTest, "tpr","fpr")
# Plot
plot(ROCRperfTest, colorize=TRUE, print.cutoffs.at=seq(0,1,.1), text.adj=c(-.2, 1.7))
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
auc
#Sensitivity (True positive rate)
tpr <- cm[2,2] / (cm[2,1]+cm[2,2])
#Specificity (true negitive rate)
tnr <- cm[1,1] / (cm[1,1]+cm[1,2])
#False Positive Rate
fpr <- 1-tnr
# False Negitive rate
fnr <- 1-tpr
rbind(TruePositiveRate=tpr,FalseNegativeRate=fnr,TrueNegativeRate=tnr,FalsePositiveRate=fpr)
source('~/GitHub/MyWork/EdExUnit3InCLass.R', echo=TRUE)
line(0.0,1.1)
?line
abline(coef=c(0,1))
# The AUC of a model has the following nice interpretation: given a random patient from the dataset who
# actually received poor care, and a random patient from the dataset who actually received good care, the
# AUC is the perecentage of time that our model will classify which is which correctly.
fluTrain<-read.csv("D:/Data/flutrain.csv")
fluTest<-read.csv("D:/Data/flutest.csv")
# Load functions
source('functions.R')
# Load the data
fluTrain<-read.csv("D:/Data/flutrain.csv")
fluTest<-read.csv("D:/Data/flutest.csv")
# Load functions
source('functions.R')
# Load the data
dfTrain<-read.csv("D:/Data/flutrain.csv")
dfTest<-read.csv("D:/Data/flutest.csv")
# Clean
dfTrain<-cleanit(dfTrain)
dfTest<-cleanit(dfTest)
#Count Blanks
colSums(!is.na(dfTrain))
colSums(!is.na(dfTest))
head(dftrain)
head(dfTrain)
tail(dftrain)
tail(dfTrain)
max(dfTrain$ili)
f1<-makedf(rchisq(1000,25))
df2<-makedf(runif(1000,2,12))
df3<-makedf(rnorm(1000,100,2))
df4<-makedf(rbinom(1000,100,.62))
df5<-makedf(rpois(1000,4))
df1<-cleanit(df1)
df2<-cleanit(df2)
df3<-cleanit(df3)
df4<-cleanit(df4)
df5<-cleanit(df5)
plot(df1)
plot(df2)
plot(df3)
plot(df4)
plot(df5)
max(dfTrain$ili)
?locate
?find
?where
?locate
?when
?where
?which
which(max(dfTrain$ili))
which(dfTrain$ili=='7')
which(dfTrain$ili==max(dfTrain$ili))
303
dftrain[303,]
dfTrain[303,]
dfTrain[which(dfTrain$ili==max(dfTrain$ili)),]
hist(dfTrain$ili)
hist(dfTrain$ili)
hist(log(in$ili))
hist(log(dfTrain$ili))
plot(log(dfTrain$ili),dfTrain$queries)
dfTrain$logili<-log(dfTrain$ili)
hist(log(dfTrain$logili))
plot(log(dfTrain$logili,dfTrain$queries)
plot(log(dfTrain$logili,dfTrain$queries))
which(dfTrain$ili==max(dfTrain$ili))
plot(dfTrain$logili,dfTrain$queries)
depvar <- 'logili'
indepvars <-c('queries')
xtabs(~get(depvar) + get(indepvars), data = dftrain)
xtabs(~get(depvar) + get(indepvars), data = dfTrain)
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
fit<-lm(f1,data=dfTrain)
fit
summary(fit)
cor(f1)
cor(dfTrain)
cor(dfTrain[,2:4])
.84^2
0.84203329^2
predictTrain = predict(fit, dfTest)
predictTest = predict(fit, type="response", newdata=dftest)
# Make predictions (probabilities) on training set
predictTest = predict(fit,newdata=dfTest)
summary(predictTest)
predictTest = exp(predict(fit,newdata=dfTest))
summary(predictTest)
summary(predictTest)
predictTest
which(dfTest$week==2012-03-11)
which(dfTest$week=="2012-03-11")
dfTest$week
which(dfTest$week=="2012-03-11 - 2012-03-17")
predictTest[11,]
predictTest[11]
(dfTest[11,] -predictTest[11])/dfTest[11,]
dfTest[11,]
(dfTest[11,2] - predictTest[11])/dfTest[11,2]
(dfTest[11,2] - predictTest[11])/dfTest[11,2]
SSE = sum((predictTest - dfTest$ili)^2)
SST = sum((mean(dfTrain$ili) - dfTest$ili)^2)
R2 = 1 - SSE/SST
R2
# Compute the RMSE
RMSE = sqrt(SSE/nrow(NBA_test))
RMSE
# Compute the RMSE
RMSE = sqrt(SSE/nrow(dfTest))
RMSE
library(zoo)
dfTrain$ililag2 = lag(zoo(dfTrain$ili), -2, na.pad=TRUE)
View(dfTrain)
dfTrain$ililag2 = coredata(dfTrain$ililag2)
View(dfTrain)
plot(log(dfTrain$ililag2,dfTrain$ili))
plot(log(dfTrain$ililag2), log(dfTrain$ili))
depvar <- 'logili'
indepvars <-c('queries','ililag2')
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
fit<-lm(f1,data=dfTrain)
summary(fit)
epvar <- 'logili'
indepvars <-c('queries','ililag2')
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
#Fit the model
# fit<-glm(f1,data=dfTrain,family=binomial)
fit<-lm(f1,data=dfTrain)
summary(fit)
dfTrain$ililag2 = lag(zoo(dfTrain$ili), -2, na.pad=TRUE)
fit<-lm(f1,data=dfTrain)
summary(fit)
Dep and Independent Vars define columns we will be working with
depvar <- 'logili'
indepvars <-c('queries')
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
#Fit the model
# fit<-glm(f1,data=dfTrain,family=binomial)
fit<-lm(f1,data=dfTrain)
summary(fit)
dfTest$ililag2 = lag(zoo(dfTest$ili), -2, na.pad=TRUE)
View(dfTest)
length(dfTrain)
dfTrain$ili[417]
dfTrain$ili[416]
dfTrain$ili[418]
dfTest$ililag2[1] = dfTrain$ili[416]
dfTest$ililag2[2] = dfTrain$ili[417]
# Dep and Independent Vars define columns we will be working with
depvar <- 'logili'
indepvars <-c('queries','ililag2')
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
#Fit the model
# fit<-glm(f1,data=dfTrain,family=binomial)
fit<-lm(f1,data=dfTrain)
summary(fit)
# Dep and Independent Vars define columns we will be working with
depvar <- 'logili'
indepvars <-c('queries','ililag2')
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
#Fit the model
# fit<-glm(f1,data=dfTrain,family=binomial)
fit<-lm(f1,data=dfTrain)
summary(fit)
# Make predictions (probabilities) on training set
predictTest = exp(predict(fit,newdata=dfTest))
# Analyze predictions
summary(predictTest)
# Compute out-of-sample R^2
SSE = sum((predictTest - dfTest$ili)^2)
SST = sum((mean(dfTrain$ili) - dfTest$ili)^2)
R2 = 1 - SSE/SST
R2
# Compute the RMSE
RMSE = sqrt(SSE/nrow(dfTest))
RMSE
summary(fit)
SSE = sum((predictTest - dfTest$ili)^2)
SST = sum((mean(dfTrain$ili) - dfTest$ili)^2)
R2 = 1 - SSE/SST
R2
