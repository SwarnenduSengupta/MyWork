sum(CovPatt$yj)
sum(LOWBWT$LOW)
```
# Logistic model and deciles of risk
The deciles of risk described on pages 6-8 of the lecture pdf are computed from the predicted probabilities of the logistic model. Thus, let us fit the model to our data:
```{r}
glmLOWBWT = glm(LOW ~  LWT + RACE, family = binomial(link = "logit"), data = LOWBWT)
(summ1 = summary(glmLOWBWT))
```
The predicted probabilities can be obtained with the `fitted` function (we have seen other ways in the R code for previous lectures). Let's add them to the data and  see how they look like:
```{r}
LOWBWT$fit = fitted(glmLOWBWT)
head(LOWBWT$fit)
tail(LOWBWT$fit)
```
To define the deciles we are going to use the `quantile` function. This is always a delicate step, since there are many methods to estimate the quantiles of a data set. In fact, the `quantile` functions has an argument called `type` which can be used to select between 9 different methods. See the [help file for `quantile`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html). In order to reproduce the results of Stata I have used `type = 6`. A different method can lead to different values of the risk deciles (experiment this by yourself, changing the value of `type`).
```{r}
(cutPoints = quantile(fitted(glmLOWBWT),probs = (0:10)/10, type = 6))
```
Now that the cut points for the deciles have been selected we use `cut` to define a factor `riskDecile` that classifies the values of the fitted probabilities into decilic classes. Then we add the factor to the data frame. Here again we have to be careful. The `cut` function has two optional arguments that determine the resulting classification:
- The argument `right` is used to decide whether the decilic classes are right-closed (and left-open) intervals or viceversa.
- The argument `include.lowest` makes the first (or last, depending on `right`) interval be closed, to include the boundary points of the data set.
In the code below these arguments are set to reproduce the results obtained with Stata that appear in the lecture pdf. If you want to reproduce the Systat results change `right=TRUE` to `right=FALSE`.
```{r}
LOWBWT$riskDecile = cut(fitted(glmLOWBWT), breaks = cutPoints, include.lowest = TRUE, right = TRUE)
table(LOWBWT$riskDecile)
```
Next the values of `LOW` for the observations in each decilic class are sumed to obtain the observed values of $y=1$. The `tapply` function is the tool for this job, applying an operation (the sum in this case) across the values of a factor.
```{r}
(Obs1 = with(LOWBWT, tapply(LOW, INDEX = riskDecile, FUN = sum)) )
```
The same operation, this time applied to the fitted probabilities gives the expected values of $y=1$.
```{r}
(Exp1 = with(LOWBWT, tapply(fit, INDEX = riskDecile, FUN = sum)) )
```
And since we know the number of observations in each decilic class, the observed and expected values for $y = 0$ are obtained by subtracting:
```{r}
(Obs0 = table(LOWBWT$riskDecile) - Obs1)
(Exp0 = table(LOWBWT$riskDecile) -Exp1)
```
We can put all of the above values in a table like the one on page 21 of the lecture pdf with this commands.
```{r}
tablePg21 = data.frame(
Prob = levels(LOWBWT$riskDecile),
Obs1 = as.vector(Obs1),
Exp1 = as.vector(Exp1),
Obs0 = as.vector(Obs0),
Exp0 = as.vector(Exp0),
Total = as.vector(table(LOWBWT$riskDecile) )
)
row.names(tablePg21) = NULL
```
I will use the `xtable` library to get the table in a fancier HTML format (you can use it to get LaTeX format as well).
```{r results='asis', eval=FALSE, comment=NULL}
library(xtable)
print(xtable(tablePg21, align = "ccccccc"), type="html", comment=FALSE)
```
And you can play with CSS to fiddle with the column width, etc. Here I will go with the defaults:
```{r results='asis', echo=FALSE}
library(xtable)
print(xtable(tablePg21, align = "ccccccc"), type="html", comment=FALSE)
```
#### Plot of the observed vs expected pairs
We can use the above results to get this plot. I have labeled the points with the number of the correspondng decilic class to make it easier to check which ones fall far from the diagonal line.
```{r fig.align='center', fig.width=5, fig.height=5}
(maxX = max(c(range(Obs1), range(Exp1))))
plot(c(0, maxX), c(0, maxX), asp=1, type="l", xlab="Observed y=1", ylab="Expected y=1")
points(Obs1, Exp1, asp=1)
text(Obs1, Exp1, 1:10, pos=4, col="firebrick")
abline(a = 0, b=1)
```
# Pearson Chi-Square Statistic
To obtain the Pearson $\chi^2$ statistic we begin by constructing the fitted values $\hat{y}_j$ for each covariate pattern. In order to do this we can apply the `predict` function to the `CovPatt` data frame that we created before (R will choose the relevant variables by their names). As we have already seen in previous lectures, when we use the option `response` the predicted values are the probabilities $\hat{\pi}_j$.
```{r}
CovPatt$prob = predict(glmLOWBWT, newdata = CovPatt, type = "response")
```
The $m_j$ are stored in the `n` variable of `CovPatt`, so the $\hat{y}_j$ are obtained as follows:
```{r}
CovPatt$hat_yj = with(CovPatt, n * prob)
```
And now the Pearson residuals $r_j$  are:
```{r}
CovPatt$residual = with(CovPatt, (yj - hat_yj)/sqrt(hat_yj * (1 - prob)))
```
Finally, the Pearson $\chi^2$ statistic is:
```{r}
(Pearson.statistic = sum((CovPatt$residual)^2))
```
You can check this result against the Stata output on page 21 of the lecture pdf. The degrees of freedom are:
```{r}
(p = length(glmLOWBWT$coefficients) - 1)
(J = length(CovPatt$n))
(Pearson.df = J - (p + 1))
```
Here $p$ is the number of variables in the logistic model (and remember that each dummy variables is counted individually). The p-value is therefore (again, check with page 21 in the pdf):
```{r}
(Pearson.pvalue = pchisq(Pearson.statistic, df = Pearson.df, lower.tail = FALSE))
```
However, as we have been warned in the lecture, it is not a good idea to use this test to decide about the goodness of fit of our model.So let's turn to something better.
# Hosmer-Lemeshow test
To define the Hosmer - Lemeshow statistic we will use the first grouping strategy described in the lecture pdf. This means that we can directly use the $2\times J$ contingency table of observed and expected values for decilic classes that we constructed before. The Hosmer - Lemeshow statistic is:
```{r}
(HL.statistic = sum((Obs1 - Exp1)^2/Exp1) + sum((Obs0 - Exp0)^2/Exp0))
```
Check against the Stata result on page 21. The degrees of freedom are:
```{r}
ROCcurve = performance( LOWBWT.pred, measure="sens", x.measure="fpr")
plot(c(0, 1), c(0, 1), xlab="Probability cutoff", ylab="Sensitivity / Specificity", type="n", asp=1)
points(cutoffs, specificity, type="l", col="firebrick")
points(cutoffs, specificity, pch=".", cex=6, col="firebrick")
points(cutoffs, sensibility, type="l", col="dodgerblue4")
points(cutoffs, sensibility, pch=".", cex=6, col="dodgerblue4")
segments(x0 = max(cutoffs[cutoffs!=Inf]), y0 = 1, x1 = 1, y1 = 1, col="firebrick")
segments(x0 = max(cutoffs[cutoffs!=Inf]), y0 = 0, x1 = 1, y1 = 0, col="dodgerblue4")
points(1, 0, cex=6, pch=".", col="dodgerblue4")
points(1, 1, cex=6, pch=".", col="firebrick")
legend(x="right", legend=c("Specificity", "Sensitivity"),
col = c("red", "blue"), bty=1, lwd=3,cex=1.5)
```
o
ROCcurve = performance( LOWBWT.pred, measure="sens", x.measure="fpr")
sens = performance(LOWBWT.pred, measure = "sens")
```
Before going further, some remarks are in order. R, as an object oriented language, has two systems of classes, called S3 and S4. The result of calling `performance` ia an object of type S4, consisting of *slots*. We can access these slots using \@ (instead of the familiar \$). For example, to access the cutoff values we use:
```{r}
cutoffs = sens@x.values[[1]]
head(cutoffs)
```
Similarly for the sensibility values themselves:
source('~/.active-rstudio-document', echo=TRUE)
LOWBWT = read.csv("D:/Data/LOWBWT.csv")
LOWBWT$SMOKE = factor(LOWBWT$SMOKE)
LOWBWT$RACE = factor(LOWBWT$RACE)
source('~/.active-rstudio-document', echo=TRUE)
LOWBWT = read.csv("D:/Data/LOWBWT.csv")
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
(Y = LOWBWT$LOW)
(LWD = factor(ifelse(LOWBWT$LWT >= 110, yes = 0, no = 1)))
(AGE = LOWBWT$AGE)
LOWBWT = data.frame(Y, LWD, AGE)
(LOWBWT = data.frame(Y, LWD, AGE)
LOWBWT = data.frame(Y, LWD, AGE)
(Y = LOWBWT$LOW)
(LWD = factor(ifelse(LOWBWT$LWT >= 110, yes = 0, no = 1)))
(AGE = LOWBWT$AGE)
LOWBWT = data.frame(Y, LWD, AGE)
LOWBWT = read.csv("D:/Data/LOWBWT.csv")
```
But since we are only going to use three variables, I will redefine the data set. Besides, the `LWD` is defined as a factor from the `LWT` variable in the original data.
```{r}
(Y = LOWBWT$LOW)
(LWD = factor(ifelse(LOWBWT$LWT >= 110, yes = 0, no = 1)))
(AGE = LOWBWT$AGE)
LOWBWT = data.frame(Y, LWD, AGE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
mean(1,23,45,12,32,78,100, 43,68, 89,68)
mean(1,23,45,12,32,78,100,43,68, 89,68)
mean(c(1,23,45,12,32,78,100,43,68,89,68))
# Load functions
source('functions.R')
# Load the data
emails = read.csv("D:/Data/energy_bids.csv", stringsAsFactors=FALSE)
str(emails)
emails$email[1]
str(emails$email[1)]
str(emails$email[1)])
strwrap(emails$email[1)])
strwrap(emails$email[1])
strwrap(emails$email[2])
table(emails$responsive)
library(tm)
library(SnowballC)
# Preprocess
corpus = Corpus(VectorSource(emails$email)) # Create corpus
corpus # Look at corpus
writeLines(as.character(corpus[[1]])) # Look at corpus
corpus <-tm_map(corpus, content_transformer(tolower)) # Convert to lower-case
corpus = tm_map(corpus, removePunctuation) # Remove punctuation
#stopwords("english")[1:200] # Look at stop words
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english"))) # Remove stopwords and apple
corpus = tm_map(corpus, stemDocument) # Stem document
writeLines(as.character(corpus[[1]]))
corpus[[1]]
emails$email[1]
corpus$email[1]
writeLines(as.character(corpus[[1]]))
writeLines(as.character(strwrap(corpus[[1]])))
# Create matrix
dtm = DocumentTermMatrix(corpus)
dtm
# Remove sparse terms
dtm = removeSparseTerms(dtm, 0.97)
dtm
# Create data frame
labeledTerms = as.data.frame(as.matrix(dtm))
# Add in the outcome variable
labeledTerms$responsive = emails$responsive
str(labeledTerms)
dtm
frequencies = DocumentTermMatrix(corpus) # Create matrix
frequencies # view  matrix
frequencies = removeSparseTerms(frequencies, 0.97) # Remove sparse terms (remove 97 of the terms%)
frequencies # view sparse terms
Bag of words
dtm = DocumentTermMatrix(corpus) # Create matrix
dtm # view  matrix
dtm = removeSparseTerms(dtm, 0.97) # Remove sparse terms (remove 97 of the terms%)
dtm # view sparse terms
df = as.data.frame(as.matrix(dtm)) #Convert to a data frame
View(df)
colnames(df) = make.names(colnames(df)) # Make all variable names R-friendly
df$responsive <-emails$responsive # Add dependent variable
df <-cleanit(df)
colnames(df) = make.names(colnames(df))
# Split the data to  build models
library(caTools)
set.seed(144)
split = sample.split(df$responsive, SplitRatio = 0.7)
dfTrain = subset(df, split==TRUE)
dfTest = subset(df, split==FALSE)
colnames(dfTrain) = make.names(colnames(dfTrain))
colnames(dfTest) = make.names(colnames(dfTest))
# clean things up
rm(df, emails,corpus,dtm,split)
corprm(frequencies)
rm(frequencies)
rm(labeledTerms)
# Build a CART model
library(rpart)
library(rpart.plot)
CARTTrain = rpart(responsive ~ ., data=dfTrain, method="class")
prp(CARTTrain)
# Determine the Majority
bl <-table(dfTrain$responsive)
majority<-ifelse(bl[1]>bl[2],0,1)
# Fill in a prediction for the majority of the training set
predictTrainBase <-rep(majority,nrow(dfTrain))
#Compare
cm <- table(dfTrain$responsive,predictTrainBase,exclude=NULL)
addmargins(cm)
getstats(cm)
predictCARTTrain = predict(CARTTrain,type="class")
cm<-table(dfTrain$responsive, predictCARTTrain)
addmargins(cm)
getstats(cm)
# Baseline on Testing data
# Determine the Majority
bl <-table(dfTest$responsive)
majority<-ifelse(bl[1]>bl[2],0,1)
# Fill in a prediction for the majority
predictTestBase <-rep(majority,nrow(dfTest))
#Compare
cm <- table(dfTest$responsive,predictTestBase,exclude=NULL)
addmargins(cm)
getstats(cm)
# Evaluate the performance of the model on TEST SET
predictCARTTest = predict(CARTTrain, newdata=dfTest, type="class")
cm<-table(dfTest$responsive, predictCARTTest)
thres=0.5
addmargins(cm)
getstats(cm)
cm<-table(dfTest$responsive, predictCARTTest)
addmargins(cm)
library(ROCR)
predROCR = prediction(pred.prob, dfTest$responsive)
predROCR = prediction(CARTTrain, dfTest$responsive)
predROCR = prediction(predictCARTTest, dfTest$responsive)
predROCR = prediction(predictCARTTest[,2], dfTest$responsive)
predROCR = prediction(predictCARTTest[,2], dfTest$responsive)
predictCARTTest[,2]
predictCARTTest
predictCARTTest[2]
predictCARTTest[,2]
predictCARTTest[1,2]
predictCARTTest[12]
predictCARTTest = predict(CARTTrain, newdata=dfTest)
cm<-table(dfTest$responsive, predictCARTTest)
thres=0.5
predictCARTTest[]
predROCR = prediction(predictCARTTest[,2], dfTest$responsive)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
# Compute AUC
performance(predROCR, "auc")@y.values
plot(ROCRperfTest, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
abline(coef=c(0,1))
plot(perfROCR, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
abline(coef=c(0,1))
performance(predROCR, "auc")@y.values
text(0.5, 1, "AUC:")
text(0.6,1, round(auc,4))
auc<-performance(predROCR, "auc")@y.values
text(0.5, 1, "AUC:")
text(0.6,1, round(auc,4))
auc
# Compute AUC
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
text(0.5, 1, "AUC:")
text(0.6,1, round(auc,4))
# Compute AUC
auc = as.numeric(performance(predROCR, "auc")@y.values)
text(0.5, 1, "AUC:")
text(0.6,1, round(auc,4))
# Evaluate the performance of the model on TEST SET
predictCARTTest = predict(CARTTrain, newdata=dfTest)
cm<-table(dfTest$responsive, predictCARTTest)
thres=0.2
addmargins(cm)
getstats(cm)
predictCARTTest = predict(CARTTrain, newdata=dfTest)
cm<-table(dfTest$responsive, predictCARTTest)
thres=0.15
predictCARTTest = predict(CARTTrain, newdata=dfTest)
cm<-table(dfTest$responsive, predictCARTTest)
thres=0.15
addmargins(cm)
getstats(cm)
# ROC curve
library(ROCR)
predROCR = prediction(predictCARTTest[,2], dfTest$responsive)
plot(, colorize=TRUE)
plot(perfROCR, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
abline(coef=c(0,1))
# Compute AUC
auc = as.numeric(performance(predROCR, "auc")@y.values)
text(0.5, 1, "AUC:")
text(0.6,1, round(auc,4))
library(randomForest)
set.seed(144)
forestFit = randomForest(responsive~., data = dfTrain)
PredictForest = predict(forestFit, newdata = dfTest)
cm <- table(dfTest$responsive, PredictForest)
addmargins(cm)
getstats(cm)
?randomForest
vu = varUsed(forestFit, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(forestFit$forest$xlevels[vusorted$ix]))
# Load functions
source('functions.R')
library(plyr)
library(ggplot2)
library(gridExtra)
library(caret)
library(tm)
library(RWeka)
library(R.utils)
library(stringi)
library(stringr)
library(SnowballC)
library(RColorBrewer)
library(magrittr)
library(wordcloud)
library(textcat)
library(xtable)
library(markovchain)
install.packages("markovchain")
install.packages("wordcloud")
install.packages("textcat")
# Load functions
source('functions.R')
library(plyr)
library(ggplot2)
library(gridExtra)
library(caret)
library(tm)
library(RWeka)
library(R.utils)
library(stringi)
library(stringr)
library(SnowballC)
library(RColorBrewer)
library(magrittr)
library(wordcloud)
library(textcat)
library(xtable)
library(markovchain)
fileLineNum = CalculateTextFileLines("D:/DataCapstone/data/en_US.blogs.txt")
fileWordCount = CountWordsInTextFile("D:/DataCapstone/data/en_US.blogs.txt")
blackList = read.csv("D:/DataCapstone/data/Terms-to-Block.csv", skip=4)
blackList = blackList[,2]
blackList = gsub(",","",blackList)
setwd("C:/Users/bryan_000/Documents/GitHub/Capstone2/App4/")
source("Utilities.R")
fileLineNum = CalculateTextFileLines("D:/DataCapstone/data/en_US.blogs.txt")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
setwd("~/GitHub/MyWork")
source('~/.active-rstudio-document', echo=TRUE)
a<- c(0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0)
b<- c(0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0)
c<-a-b
c
d=c*c
d
sqrt(2)
plot (a,b)
scatterplot(a,b)
scatter(a,b)
# Load functions
source('functions.R')
# After following the steps in the video, load the data into R
# After following the steps in the video, load the data into R
movies = read.table(""D:/Data/emovieLens.txt", header=FALSE, sep="|",quote="\"")
movies = read.table("D:/Data/emovieLens.txt", header=FALSE, sep="|",quote="\"")
m
e
movies = read.table("D:/Data/u.txt", header=FALSE, sep="|",quote="\"")
str(movies)
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
str(movies)
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
movies = unique(movies)
# Take a look at our data again:
str(movies)
# Load functions
source('functions.R')
# Load the data into R
movies = read.table("D:/Data/u.txt", header=FALSE, sep="|",quote="\"")
str(movies)
# Add column names
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
# Remove unnecessary variables
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
# Remove duplicates
movies = unique(movies)
# Take a look at our data again:
str(movies)
table (movies)
table (movies$Comedy)
table (movies$Western)
table (movies$Romance, movies$Drama)
distances = dist(movies[2:20], method = "euclidean")
# Hierarchical clustering
clusterMovies = hclust(distances, method = "ward")
plot(clusterMovies)
clusterGroups = cutree(clusterMovies, k = 10)
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
tapply(movies$Adventure, clusterGroups, mean)
tapply(movies$Animation, clusterGroups, mean)
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
tapply(movies$Adventure, clusterGroups, mean)
tapply(movies$Animation, clusterGroups, mean)
tapply(movies$Childrens, clusterGroups, mean)
tapply(movies$Comedy, clusterGroups, mean)
tapply(movies$Crime, clusterGroups, mean)
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
tapply(movies$Adventure, clusterGroups, mean)
tapply(movies$Animation, clusterGroups, mean)
tapply(movies$Childrens, clusterGroups, mean)
tapply(movies$Comedy, clusterGroups, mean)
tapply(movies$Crime, clusterGroups, mean)
tapply(movies$Documentary, clusterGroups, mean)
tapply(movies$Drama, clusterGroups, mean)
tapply(movies$Fantasy, clusterGroups, mean)
tapply(movies$FilmNoir, clusterGroups, mean)
tapply(movies$Horror, clusterGroups, mean)
tapply(movies$Musical, clusterGroups, mean)
tapply(movies$Mystery, clusterGroups, mean)
tapply(movies$SciFi, clusterGroups, mean)
tapply(movies$Thriller, clusterGroups, mean)
tapply(movies$War, clusterGroups, mean)
tapply(movies$western, clusterGroups, mean)
tapply(movies$Western, clusterGroups, mean)
subset(movies, Title=="Men in Black (1997)")
clusterGroups[257]
cluster2 = subset(movies, clusterGroups==2)
# Look at the first 10 titles in this cluster:
cluster2$Title[1:10]
cluster2$Title[1:]
cluster2$Title[1:10]
cluster2$Title
# Assign points to clusters
twoGroups = cutree(clusterMovies, k = 2)
twoGroups
tapply(movies$Action, twoGroups, mean)
tapply(movies$Romance, twoGroups, mean)
tapply(movies$Adventure, twoGroups, mean)
tapply(movies$Animation, twoGroups, mean)
tapply(movies$Childrens, twoGroups, mean)
tapply(movies$Comedy, twoGroups, mean)
tapply(movies$Crime, twoGroups, mean)
tapply(movies$Documentary, twoGroups, mean)
tapply(movies$Drama, twoGroups, mean)
tapply(movies$Fantasy, twoGroups, mean)
tapply(movies$FilmNoir, twoGroups, mean)
tapply(movies$Horror, twoGroups, mean)
tapply(movies$Musical, twoGroups, mean)
tapply(movies$Mystery, twoGroups, mean)
tapply(movies$SciFi, twoGroups, mean)
tapply(movies$Thriller, twoGroups, mean)
tapply(movies$War, twoGroups, mean)
tapply(movies$Western, twoGroups, mean)
library(rUnemploymentData)
data(df_county_unemployment)
ylab="Percent Unemployment")
boxplot(df_county_unemployment[, c(-1, -2, -3)],main="USA County Unemployment Data",xlab="Year",ylab="Percent Unemployment")
county_unemployment_choropleth(year=2013)
animated_county_unemployment_choropleth()
county_unemployment_choropleth(year=2009)
